[{
  "tag": "H2",
  "text": "Bibliography",
  "translation": "参考书目"
}, {
  "tag": "P",
  "text": "[1] Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. “Imagenet classification with deep convolutional neural networks.” Advances in neural information processing systems. 2012.",
  "translation": "[1] Krizhevsky，Alex，Ilya Sutskever和Geoffrey E. Hinton。 “具有深度卷积神经网络的Imagenet分类。” 神经信息处理系统的进步。 2012。"
}, {
  "tag": "P",
  "text": "[2] Jozefowicz, Rafal, et al. “Exploring the limits of language modeling.” arXiv preprint arXiv:1602.02410 (2016).",
  "translation": "[2] Jozefowicz，Rafal等。 “探索语言建模的局限性。” arXiv预印本arXiv：1602.02410（2016）。"
}, {
  "tag": "P",
  "text": "[3] Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. “Sequence to sequence learning with neural networks.” Advances in neural information processing systems. 2014.",
  "translation": "[3] Sutskever，Ilya，Oriol Vinyals和Quoc V. Le。 “使用神经网络进行序列学习的序列。” 神经信息处理系统的进步。 2014。"
}, {
  "tag": "P",
  "text": "[4] LeCun, Yann, et al. “Handwritten digit recognition with a back-propagation network.” Advances in neural information processing systems. 1990."
}, {
  "tag": "P",
  "text": "[5] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. “Learning representations by back-propagating errors.” nature 323.6088 (1986): 533–536."
}, {
  "tag": "H1",
  "text": "Questions",
  "translation": "问题"
}, {
  "tag": "H2",
  "text": "What’s the need for a bias term b?",
  "translation": "偏项b有什么需要？"
}, {
  "tag": "P",
  "text": "The bias term is easiest to understand if we can visualise what it is doing. First let’s use the sigmoid function as our nonlinear activation function. Let’s also consider a simplified network where there is a single weight w₁ and a single input from the previous layer x₁.",
  "translation": "如果我们可以直观地看到偏见，那么最容易理解它。 首先，让我们使用S型函数作为非线性激活函数。 我们还考虑一个简化的网络，其中只有一个权重w₁和来自上一层x₁的一个输入。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*TE5yiBDi1hUUgQ8SfXdXcg.png?q=20",
  "caption": "Figure 9: Activation of the first neuron in the second layer for various values of the bias term",
  "type": "image",
  "file": "1!TE5yiBDi1hUUgQ8SfXdXcg.png"
}, {
  "tag": "P",
  "text": "On the left of fig. 9 is the sigmoid function for 3 different values of the single weight w₁. You can see that if the NN changes the weight, this will simply change the steepness of the graph.",
  "translation": "在图的左边。 图9是针对单个权重w 1的3个不同值的S形函数。 您可以看到，如果NN改变了权重，这只会改变图表的陡度。"
}, {
  "tag": "P",
  "text": "What about if when x₁ = 2, which gives a very strong activation for this neuron, we get a high cost function C(w). The network then decides that actually making this neuron’s activation close to 0 is good for C(w). From changing the weight alone it would never be able to make the activation of this neuron 0. This is where the bias term comes in."
}, {
  "tag": "P",
  "text": "From the right side of fig. 9 you can see that the bias term acts to shift the whole function along the x axis, but doesn’t change the overall shape of it. Therefore, you can now see that for an input of 2 you can get an activation close to 0 if b₁ > 0. Similarly, you can get the reverse situation if you’re bias term is negative."
}, {
  "tag": "P",
  "text": "Essentially, the bias term gives the NN more degrees of freedom with which to fit the data you have supplied it with."
}, {
  "tag": "H2",
  "text": "What’s the need for a nonlinear activation function σ?"
}, {
  "tag": "P",
  "text": "The short answer: without a nonlinear function, the multiplication of many weights and activations is simply a linear function of the input. Thus you can only solve problems that are linearly separable.",
  "translation": "简短的答案：没有非线性函数，许多权重和激活的乘积就是输入的线性函数。 因此，您只能解决线性可分离的问题。"
}, {
  "tag": "P",
  "text": "The long answer: what do we mean by linearly separable? This means that your problem can be solved using a hyperplane. A hyperplane in 2 dimensions is a.k.a a line, in 3 dimensions it is a.k.a a plane (like a piece of paper), and in general we call it a hyperplane.",
  "translation": "答案很长：线性可分离是什么意思？ 这意味着可以使用超平面解决您的问题。 二维的超平面也就是一条线，三维的超平面也就是一个平面（就像一张纸），通常我们称之为超平面。"
}, {
  "tag": "P",
  "text": "For example, consider the left hand side of fig. 10 which contains two distinct groups coloured in green and orange. The blue line shows a possible hyperplane that solves this classification task. This is an example of a linearly separable problem",
  "translation": "例如，考虑图5的左侧。 10包含两个分别用绿色和橙色上色的组。 蓝线显示了解决此分类任务的可能超平面。 这是线性可分问题的一个例子"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*-E0rxQIGeL5py3iiHnRxhA.png?q=20",
  "caption": "Figure 10: (left) linear classification problem and (right) non-linear classification problem",
  "type": "image",
  "file": "1!-E0rxQIGeL5py3iiHnRxhA.png"
}, {
  "tag": "P",
  "text": "Now consider a problem which is not linearly separable. The easiest example of such a problem is the XOR gate. It has two inputs, let’s call them x₁ and x₂ and one output. The XOR problem is shown on the right-hand side of fig. 10. There is no hyperplane that perfectly separates out the two classes in this scenario.",
  "translation": "现在考虑一个不可线性分离的问题。 此类问题最简单的示例是XOR门。 它有两个输入，我们称它们为x₁和x²，还有一个输出。 XOR问题显示在图的右侧。 10.在这种情况下，没有超平面完美地将这两个类别分开。"
}, {
  "tag": "P",
  "text": "How does this relate to the nonlinear function of the NN? Without the nonlinear function, the many layers of weight and activation multiplications could be represented by one big matrix M that maps the input vector, X to the output vector Y (i.e. M.X = Y). This matrix would be linear and thus would not be able to solve the XOR problem. Handwritten digit recognition is another such example of a non-linear problem. So, the nonlinear function breaks the linearity of NN so that they are able to solve more complex problems.",
  "translation": "这与NN的非线性函数有何关系？ 没有非线性函数，权重和激活乘法的多层可以由一个大矩阵M表示，该矩阵将输入向量X映射到输出向量Y（即M.X = Y）。 该矩阵将是线性的，因此将无法解决XOR问题。 手写数字识别是非线性问题的另一个示例。 因此，非线性函数破坏了NN的线性，因此它们能够解决更复杂的问题。"
}, {
  "tag": "H2",
  "text": "Summary",
  "translation": "摘要"
}, {
  "tag": "P",
  "text": "So to train our NN we need to perform forward propagation of all our training images. Calculate the average cost, C(w). Then use back propagation to calculate the derivative of C(w) with respect to all of the weights in the NN. Finally, use the update equation to use gradient descent to gradually move the weights to the minimum of C(w). Simple!",
  "translation": "因此，要训练我们的神经网络，我们需要对所有训练图像进行正向传播。 计算平均成本C（w）。 然后使用反向传播来计算C（w）关于NN中所有权重的导数。 最后，使用更新方程式使用梯度下降法将权重逐渐移至C（w）的最小值。 简单！"
}, {
  "tag": "P",
  "text": "The only thing I have not presented here is exactly how to calculate the gradient of C(w) as it is a little more involved and it becomes unavoidable to include some maths that can look quite intimidating. Despite the form of the equations I think they are quite simple, involving nothing more than taking derivatives and using the chain rule which we all learnt in secondary school calculus class. However, I shall leave that for a separate post."
}, {
  "tag": "P",
  "text": "I hope you enjoyed learning about the internals of NNs. See you in the next post.",
  "translation": "我希望您喜欢学习NN的内部知识。 下篇文章见。"
}, {
  "tag": "P",
  "text": "Stay curious.",
  "translation": "保持好奇心。"
}, {
  "tag": "H2",
  "text": "Back Propagation",
  "translation": "反向传播"
}, {
  "tag": "P",
  "text": "To start adjusting our weights and biases to get the NN closer to the correct answer the first thing we need to do is define a cost function. A cost function just quantifies how wrong a system is. Let’s use mean squared error (MSE). To calculate the MSE for a single training example, we run our digit through our NN and calculate the difference between the activation of the neurons in the output layer and the desired activation. We then square these values, add them all up, and divide by the number of terms we added up which is 10 for our case."
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*gjuZsvJw0MyZvOhk4jsw7A.png?q=20",
  "caption": "Figure 6: Image shows the calculation of the cost for a single training example",
  "type": "image",
  "file": "1!gjuZsvJw0MyZvOhk4jsw7A.png"
}, {
  "tag": "P",
  "text": "Fig. 6 shows the calculation of the cost function for a training example of the digit 0. This is the cost of a single training example. Then we run the rest of our training examples through the network and calculate all of their cost’s. We define a single number, called the average cost, to be the average of all the single example cost’s. For example, if we had 400 training examples, our average cost would be the sum of all the costs of the 400 examples, divided by 400. This gives us the average cost for 1 round of training (usually called an epoch), let’s represent this by C(w)."
}, {
  "tag": "P",
  "text": "Now we want to change all of the weights and biases, so that this cost is as small as possible.",
  "translation": "现在我们要更改所有权重和偏差，以使此成本尽可能小。"
}, {
  "tag": "P",
  "text": "To better understand this, imagine the whole system is characterised by a single weight, instead of the thousands that we have at the moment. Then our cost function might look something like Fig. 7.",
  "translation": "为了更好地理解这一点，可以想象整个系统的特征是单一的权重，而不是我们目前拥有的数千。 那么我们的成本函数可能类似于图7。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*0358XEAalVpUKNnyOcpjmw.png?q=20",
  "caption": "Figure 7: Graph showing the cost function C(w) (red) and two possible values of w (blue circles) along with their gradients (black dashed)",
  "type": "image",
  "file": "1!0358XEAalVpUKNnyOcpjmw.png"
}, {
  "tag": "P",
  "text": "Plotted in red is the average cost, C(w), for different values of w. As stated before, we want to minimise C(w). In other words, we want to change w so that we get to the minimum of the red curve.",
  "translation": "用红色绘制的是不同w值的平均成本C（w）。 如前所述，我们要最小化C（w）。 换句话说，我们想要改变w，以便我们达到红色曲线的最小值。"
}, {
  "tag": "P",
  "text": "Also shown in Fig. 7 are two possible values for w as blues circles with their associated gradients as black dashed lines. What we can deduce from Fig. 7 is that we should move w to the right if the gradient at our current point is negative, and to the left if the gradient is positive.",
  "translation": "图7中还显示了w作为蓝色圆圈的两个可能值，其关联的渐变为黑色虚线。 从图7可以得出的结论是，如果当前点的坡度为负，则应将w向右移动；如果坡度为正，则应将w向左移动。"
}, {
  "tag": "P",
  "text": "Furthermore, we should move by an amount that is proportional to the gradient. If the gradient is large, we should move by a larger amount than if the gradient is small as this indicates that we are close to the minimum of the red curve. This logic can be summed up by the weight update equation.",
  "translation": "此外，我们应移动与梯度成比例的量。 如果渐变较大，则移动的幅度应大于渐变较小的幅度，因为这表明我们接近红色曲线的最小值。 该逻辑可以通过权重更新方程式进行总结。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*t9WO5g5hGduv5YKZIF3eiA.png?q=20",
  "caption": "Figure 8: Weight update equation",
  "type": "image",
  "file": "1!t9WO5g5hGduv5YKZIF3eiA.png"
}, {
  "tag": "P",
  "text": "NOTE: Remember that we do not know the exact surface of C(w) as shown in Fig. 7 otherwise we could simply change our weight to the optimal value to minimise C(w). We instead have to use our local measurement of the gradient to tells us which direction to move in to get us closer to the minimum.",
  "translation": "注意：请记住，我们不知道C（w）的确切表面，如图7所示，否则我们可以简单地将权重更改为最佳值以最小化C（w）。 相反，我们必须使用梯度的局部度量来告诉我们要朝哪个方向移动，以使我们更接近最小值。"
}, {
  "tag": "P",
  "text": "The weight update equation tells us that the weights in the next training run (the next epoch) w(t+1) should be equal to the weights of the current run w(t) adjusted by the gradient dC(w)/dw of the current run. η is called the ‘learning rate’, and it controls how much influence the current gradient has on the weight update. It is usually a small value such as 0.01 or 0.001."
}, {
  "tag": "P",
  "text": "This process of calculating the gradient of your current position and then moving by an amount proportional to the gradient is called ‘gradient descent’."
}, {
  "tag": "P",
  "text": "To perform gradient descent, we need to calculate the derivative of C(w) with respect to each of the weights in our NN. The technique of finding the gradient of each of the weights in the NN is called back propagation and is the workhorse of NNs. It is called back propagation because you start by calculating the gradient of C(w) with respect to the weights in the output layer. Then the gradient of C(w) with respect to the layer before the output layer depends on the gradients of the output layer. In this way, you are “back propagating” your calculations of the gradient to work out the gradient of weights further and further back into your network."
}, {
  "tag": "H2",
  "text": "Forward Propagation",
  "translation": "正向传播"
}, {
  "tag": "P",
  "text": "To give the NN some context we will consider one of the original problems it was applied to - handwritten digit recognition [4]. This problem is often one of the first encountered by students of Deep Learning as it represents a problem that is hard to solve with traditional machine learning methods.",
  "translation": "为了给NN一些上下文，我们将考虑将其应用于原始问题之一-手写数字识别[4]。 这个问题通常是深度学习的学生首先遇到的一个问题，因为它代表着传统机器学习方法难以解决的问题。"
}, {
  "tag": "P",
  "text": "Imagine that we are tasked to write a computer program that can identify handwritten digits. Each image we receive will be 28 x 28 pixels and we will also have access to the correct label for that image. The first thing we do is set up our neural network as in the figure below.",
  "translation": "想象一下，我们要编写一个可以识别手写数字的计算机程序。 我们收到的每个图像将为28 x 28像素，我们还将可以访问该图像的正确标签。 我们要做的第一件事是建立我们的神经网络，如下图所示。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*b4RN6LZQF2XK5seI-bfz4A.png?q=20",
  "caption": "Figure 1: Generic NN",
  "type": "image",
  "file": "1!b4RN6LZQF2XK5seI-bfz4A.png"
}, {
  "tag": "P",
  "text": "Each circle in Fig. 1 represents a ‘neuron’. A neuron simply holds a scalar value, i.e. a number. The red neurons define the input layer. This is where we will input our image. The blue neurons represent the hidden layers. They are called hidden layers because the user never interacts directly with these neurons. The green neurons represent the output layer. This is where the NN will output its prediction for what digit is in the image.",
  "translation": "图1中的每个圆圈代表一个“神经元”。 神经元简单地持有标量值，即数字。 红色神经元定义输入层。 这是我们输入图像的地方。 蓝色神经元代表隐藏的层。 它们被称为隐藏层，因为用户永远不会直接与这些神经元交互。 绿色的神经元代表输出层。 在这里，NN将输出其对图像中数字的预测。"
}, {
  "tag": "P",
  "text": "We have 784 pixels in total (28 x 28), so our input layer consists of 784 neurons. Let’s call the top red neuron in Fig. 1 neuron 1 and the bottom one neuron 784. The value of each neuron takes on the value of the pixel it is connected to. For example, if our image has a pixel depth of 8 bit, then a single pixel can take on a value between 0 and 256 (2⁸). 0 would represent a completely white pixel and 256 would represent a completely black pixel, with a sliding scale in between these values. The value of a neuron is called it’s activation. We will represent the activation of a neuron by the symbol a.",
  "translation": "我们总共有784个像素（28 x 28），因此我们的输入层包含784个神经元。 我们将图1中的顶部红色神经元称为神经元1，将底部红色神经元称为784。每个神经元的值取其所连接像素的值。 例如，如果我们的图像的像素深度为8位，则单个像素的取值范围为0到256（2⁸）。 0将代表一个完全白色的像素，而256将代表一个完全黑色的像素，在这些值之间有一个缩放比例。 神经元的价值被称为激活。 我们将用符号a表示神经元的激活。"
}, {
  "tag": "P",
  "text": "The next thing we do is we attach every neuron in the input layer to every neuron in the next layer. This is shown as grey lines in Fig. 1. We also add a weight to each of these connections. A weight, like the neuron’s value, is just a scalar value. Fig. 2 shows three of the weights labelled for our NN. Here the subscripts just help us to keep track of which two neurons the weight corresponds to. So for instance, the weight w31 is the weight for the connection between neuron 1 in the input layer and neuron 3 in the first hidden layer. The larger the weight between neurons the stronger the connection between them.",
  "translation": "接下来要做的是将输入层中的每个神经元附加到下一层中的每个神经元。 这在图1中显示为灰色线。我们还为每个连接添加了权重。 权重就像神经元的值一样，只是一个标量值。 图2显示了为我们的NN标记的三个权重。 在这里，下标只是帮助我们跟踪重量对应的两个神经元。 因此，例如，权重w31是输入层中的神经元1和第一隐藏层中的神经元3之间的连接权重。 神经元之间的权重越大，它们之间的联系越牢固。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*pk41Ztul9KFHd4-qRDhdIg.png?q=20",
  "caption": "Figure 2: A generic NN with weights",
  "type": "image",
  "file": "1!pk41Ztul9KFHd4-qRDhdIg.png"
}, {
  "tag": "P",
  "text": "Now we need to work out the activation values of the neurons in the first hidden layer. We do this in two parts. In the first part we simply multiply the activation of each neuron in the previous layer by the weight that is attached to it. We then add all these terms up and add a ‘bias’ term which we denote b. Finally, we put this weighted sum through a nonlinear function such as tanh(x) or the sigmoid function which I will represent by σ. [See the “Questions” section if you’re interested in why we use a bias term and a nonlinear function.]",
  "translation": "现在我们需要计算出第一个隐藏层中神经元的激活值。 我们分为两个部分。 在第一部分中，我们简单地将前一层中每个神经元的激活程度乘以附着在其上的权重。 然后，我们将所有这些术语加起来，并添加一个“ bias”（偏差）术语，我们将其表示为b。 最后，我们将加权总和通过非线性函数（例如tanh（x）或S型函数）表示，我将用σ表示。 [如果您对我们为什么要使用偏置项和非线性函数感兴趣，请参阅“问题”部分。]"
}, {
  "tag": "P",
  "text": "Let’s break this down a bit. If we consider what is happening at the first neuron in the hidden layer then we have what is displayed in Fig. 3.",
  "translation": "让我们分解一下。 如果我们考虑在隐藏层的第一个神经元发生了什么，那么我们将在图3中显示。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*pledBRKKdCvS_Yx6TA6KBQ.png?q=20",
  "caption": "Figure 3: A generic NN with weights and activations labelled",
  "type": "image",
  "file": "1!pledBRKKdCvS_Yx6TA6KBQ.png"
}, {
  "tag": "P",
  "text": "This neuron has contributions from all the neurons in the first layer. Each of the neurons in the first layer come with their own weight which they are multiplied by. Then we add all these terms and add the single bias term. This weighted sum is represented by the symbol z₁² where the 1 shows that we are talking about the first neuron in the layer, and the 2 in the superscript denotes we are referring to the second layer in the network. Then this weighted sum is put through the nonlinear function to obtain the activation of the first neuron in the first hidden layer (second layer of the network), which we will call a₁².",
  "translation": "该神经元具有第一层中所有神经元的贡献。 第一层中的每个神经元都有各自的权重，乘以它们的权重。 然后，我们添加所有这些项并添加单个偏差项。 该加权和由符号z₁²表示，其中1表示我们正在谈论该层中的第一个神经元，上标2表示我们正在引用网络中的第二个层。 然后，将这个加权总和通过非线性函数进行处理，以获取第一隐藏层（网络的第二层）中第一神经元的激活，我们将其称为a₁²。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*0YendreWuMhuyNGoxn8Iog.png?q=20",
  "caption": "Figure 4: Equations defining the weighted sum and activation for the first neuron in the first hidden layer",
  "type": "image",
  "file": "1!0YendreWuMhuyNGoxn8Iog.png"
}, {
  "tag": "P",
  "text": "We repeat this process for all neurons in the first hidden layer: multiplying each of the neurons from the input later by the weight connecting it to the neuron in the first hidden layer, adding a bias term, and then putting this weighted sum z into a nonlinear function."
}, {
  "tag": "P",
  "text": "Now that we have the activations for all the neurons in the first hidden layer, we repeat this process to get the activations for the neurons in the second hidden layer and then again to get the activations for the output layer. It is important to know that the weights between different layers are different.",
  "translation": "现在我们已经获得了第一隐藏层中所有神经元的激活，我们重复此过程以获取第二隐藏层中神经元的激活，然后再次获得输出层的激活。 重要的是要知道不同层之间的权重是不同的。"
}, {
  "tag": "P",
  "text": "Since our task is to predict the digit in the image, the output layer will contain 10 neurons representing the numbers 0,1,2,…,9. Each of the neurons in the final layer will contain a activation value between 0 and 1 and we take the neuron with the highest activation as the NNs guess as to what digit is in the image."
}, {
  "tag": "P",
  "text": "Before the NN has been trained, the network will most likely not be able to correctly identify what digit is in the image. For instance, consider the situation shown in Fig. 5."
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*k6QuxXgM7CE6KG9obIdNDw.png?q=20",
  "caption": "Figure 5: Generic NN showing the output of the network for an input of the digit 7",
  "type": "image",
  "file": "1!k6QuxXgM7CE6KG9obIdNDw.png"
}, {
  "tag": "P",
  "text": "An image of the digit 7 was input into the NN and all of the activations calculated. What we see at the output layer is that the activation value for the neuron representing the digit 8 is highest and so the NN thinks that the digit 8 was in the image which clearly is wrong. So what we need to do is change the values of all the weights and all the biases so that the activations of all the neurons in the output layer are pushed towards 0 except for the neuron representing the digit 7."
}, {
  "tag": "P",
  "text": "With networks nowadays easily able to contain millions, and even billions, of weights and biases, how on Earth do we make all these changes to get us closer to the correct answer?"
}, {
  "tag": "P",
  "text": "It turns out that we know, and have done for quite some time [5], a technique which very efficiently calculates the changes needed. This algorithm is called back propagation.",
  "translation": "事实证明，我们知道并且已经完成了相当长的时间[5]，该技术可以非常有效地计算所需的更改。 该算法称为反向传播。"
}, {
  "tag": "H1",
  "text": "NN —How do they work?",
  "translation": "NN-它们如何运作？"
}, {
  "tag": "P",
  "text": "NNs contain two phases;",
  "translation": "神经网络包含两个阶段。"
}, {
  "tag": "OL",
  "texts": ["Forward Propagation", "Back Propagation"],
  "translations": ["正向传播", "反向传播"]
}, {
  "tag": "P",
  "text": "During the training of a NN you perform both of these phases. When you would like the NN to make a prediction for an unlabelled example, you would simply perform forward propagation. Back propagation is really the magic that allows NN to perform so well for tasks that are traditionally very difficult for computers to carry out.",
  "translation": "在NN训练期间，您将执行这两个阶段。 当您希望NN对未标记的示例进行预测时，只需执行正向传播。 反向传播实际上是使NN在传统上很难执行的任务上表现出色的魔力。"
}, {
  "tag": "H2",
  "text": "Data Science for All",
  "translation": "全民数据科学"
}, {
  "tag": "H1",
  "text": "A Beginners Guide to Neural Nets",
  "translation": "神经网络初学者指南"
}, {
  "tag": "H2",
  "text": "A dive into the ‘black box’",
  "translation": "深入了解“黑匣子”"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*-5ooZXxgRr032s-z4ApKDQ.png?q=20",
  "type": "image",
  "file": "1!-5ooZXxgRr032s-z4ApKDQ.png"
}, {
  "tag": "H1",
  "text": "Introduction",
  "translation": "介绍"
}, {
  "tag": "P",
  "text": "If you’re reading this article you probably already know the benefits of neural nets (NN) and deep learning but maybe you want to obtain an understanding of how they work. You’ve come to the right place!"
}, {
  "tag": "P",
  "text": "What surprised me most when learning about NNs for the first time was their mathematical simplicity. Despite NNs being on the frontier of breakthroughs in fields such as computer vision [1] and natural language processing [2,3], the inner workings of them are quite simple to understand.",
  "translation": "第一次学习NN时，最让我惊讶的是它们的数学简单性。 尽管神经网络在计算机视觉[1]和自然语言处理[2,3]等领域处于突破性的前沿，但是它们的内部结构还是很容易理解的。"
}, {
  "tag": "P",
  "text": "In this article we will go through a description of how NN work by using the “Hello World!” equivalent of deep learning — handwritten digit recognition."
}, {
  "tag": "P",
  "text": "So let’s get straight into it.",
  "translation": "因此，让我们直接开始吧。"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Shane De Silva的文章《A Beginners Guide to Neural Nets》，参考：https://towardsdatascience.com/a-beginners-guide-to-neural-nets-5cf4050117cb)"
}]