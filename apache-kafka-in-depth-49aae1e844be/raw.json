[{
  "tag": "H1",
  "text": "How consumer retrieves a message?",
  "translation": "消费者如何检索消息？"
}, {
  "tag": "P",
  "text": "The consumer also retrieves the messages in the same way producer writes it by looking up into the metadata and reads the message from the leader partition. As Kafka is very fast and can get real-time messages, a single consumer will certainly have latency in reading a big chunk of a message from a topic known as consumer lag.",
  "translation": "消费者还以生产者通过查找元数据来写入消息的方式检索消息，并从领导者分区读取消息。 由于Kafka的速度非常快并且可以获取实时消息，因此单个消费者肯定会在从称为“消费者滞后”的主题中读取大量消息中存在延迟。"
}, {
  "tag": "P",
  "text": "To overcome this problem, A Consumer group can be created which consists of several consumers having the same group id. Each consumer connects with a unique partition divided equally among all the consumers. The assignment of the partition to a particular consumer is the responsibility of Group Coordinator — One of the brokers in the cluster is nominated for this role. In order to manage the list of active consumers, all the consumer of a group sends their heartbeat to the group coordinator. The number of consumers in a group should be less than or equal to the number of partitions in that particular topic, violating the condition will end up in a situation where a consumer sits idle.",
  "translation": "为了克服此问题，可以创建一个“消费方”组，该“消费方”组由多个具有相同组标识的消费者组成。 每个使用者都连接有一个唯一的分区，该分区在所有使用者之间平均分配。 将分区分配给特定的使用者是组协调员的责任-群集中的一个经纪人被提名担任此角色。 为了管理活动使用者列表，组中的所有使用者将他们的心跳发送到组协调器。 组中使用方的数量应小于或等于该特定主题中的分区的数量，违反条件将最终导致某个用户闲置的情况。"
}, {
  "tag": "H2",
  "text": "Hands-on:",
  "translation": "动手："
}, {
  "tag": "PRE",
  "text": "Creating a simple Consumer:bin/kafka-console-consumer.sh --bootstrap-server localhost:9091 --topic topic1Creating consumer to read from particular offset and partition:bin/kafka-console-consumer.sh --bootstrap-server localhost:9091 --topic topic1 --offset 0 --partition 1Creating a consumer group:bin/kafka-console-consumer.sh --bootstrap-server localhost:9091 --topic topic1 --group groupNameMultiple process/consumer of the above groupName can be created which is called Consumer Group.",
  "translation": "创建一个简单的Consumer：bin / kafka-console-consumer.sh --bootstrap-server localhost：9091 --topic topic1创建消费者以读取特定偏移量和分区：bin / kafka-console-consumer.sh --bootstrap-server localhost ：9091 --topic topic1 --offset 0 --partition 1创建使用者组：bin / kafka-console-consumer.sh --bootstrap-server localhost：9091 --topic topic1 --group groupName上面groupName的多个进程/消费者 可以创建一个称为“消费者组”的文件。"
}, {
  "tag": "P",
  "text": "More than one consumer can read a single topic at the same time. Now, in order to remember till which offset the particular read, storage known as consumer offset as a hidden topic - __consumer_offsets is provided to store the last offset of a partition read by the consumer of a particular group.",
  "translation": "一个以上的消费者可以同时阅读一个主题。 现在，为了记住特定读取的偏移量，提供了称为使用者偏移量的存储作为隐藏主题-__consumer_offsets，用于存储特定组的使用者读取的分区的最后偏移量。"
}, {
  "tag": "P",
  "text": "The Consumer offset has the key as — > [Group Id, Topic, Partition] and value — > [Offset, …]",
  "translation": "消费者偏移的键为—> [组ID，主题，分区]和值—> [偏移，...]"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/0*c5RSGYMpEmMZjEez.png?q=20",
  "caption": "Consumer Group _ taken from here",
  "type": "image",
  "file": "0!c5RSGYMpEmMZjEez.png"
}, {
  "tag": "H2",
  "text": "Consumer API:",
  "translation": "使用者API："
}, {
  "tag": "P",
  "text": "Similar to producer API, Kafka provides classes to connect to the bootstrap servers and get the messages. Deserializer needs to be written when passing a message of other than standard data types.",
  "translation": "与生产者API相似，Kafka提供了一些类来连接到引导服务器并获取消息。 传递标准数据类型以外的消息时，需要编写反序列化程序。"
}, {
  "tag": "P",
  "text": "Java API of Kafka for Producer and Consumer can be found here: https://github.com/ercsonusharma/learnkafka",
  "translation": "可以在以下位置找到针对生产者和消费者的Kafka Java API：https：//github.com/ercsonusharma/learnkafka"
}, {
  "tag": "H1",
  "text": "How Kafka is Fast?",
  "translation": "Kafka如何快速？"
}, {
  "tag": "P",
  "text": "Kafka follows a certain strategy which is part of its design to make it perform better and faster.",
  "translation": "Kafka遵循一定的策略，这是其设计的一部分，以使其性能更好，更快。"
}, {
  "tag": "OL",
  "texts": ["No Random Disk Access: It uses a sequential data structure known as an immutable queue where read and write operation is always constant time O(1). It appends the message at the end and read from the beginning or from a particular offset.", "Sequential I/O: Modern operating systems allocate most of their free memory to disk-caching and are faster for storing and retrieving sequential data.", "Zero Copy: The data from disk is unnecessarily loaded into the application memory as it is not being modified at all. So, instead of loading it to the application, it sends the same data from the kernel context buffer over the socket, NIC buffer and to the network.", "Batching of messages: Several messages are grouped together in order to avoid the multiple network call.", "Message Compression: Before transferring the message over the wire, it is compressed using compression algorithm like gzip, snappy, etc. and decompressed at the consumer API layer."],
  "translations": ["无随机磁盘访问：它使用称为不可变队列的顺序数据结构，其中读写操作始终为恒定时间O（1）。 它在末尾附加消息，并从头开始或从特定偏移量读取。", "顺序I / O：现代操作系统将其大部分可用内存分配给磁盘缓存，并且更快地存储和检索顺序数据。", "零复制：由于根本没有修改磁盘数据，因此不必要地将它们加载到应用程序内存中。 因此，它不是将其加载到应用程序，而是通过套接字，NIC缓冲区和网络从内核上下文缓冲区发送相同的数据。", "消息的批处理：为了避免多次网络呼叫，将多个消息分组在一起。", "消息压缩：在通过网络传输消息之前，使用gzip，snappy等压缩算法对消息进行压缩，然后在使用者API层将其解压缩。"]
}, {
  "tag": "H1",
  "text": "How the data reside on the Broker instance / physical disk?",
  "translation": "数据如何驻留在Broker实例/物理磁盘上？"
}, {
  "tag": "P",
  "text": "All the messages in a broker are stored in the log directory (log-dir-1) configured in the config file before turning the Kafka server up. Inside that directory, a folder containing a partition of a particular topic can be found in the format as topic_name-partition_number e.g. topic1–0. The __consumer_offsets topic is also stored in the same log directory.",
  "translation": "在打开Kafka服务器之前，代理中的所有消息都存储在配置文件中配置的日志目录（log-dir-1）中。 在该目录中，可以找到包含特定主题分区的文件夹，格式为topic_name-partition_number，例如 主题1–0。 __consumer_offsets主题也存储在同一日志目录中。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*E3b9_nT-2PbZERMo0ArspA.png?q=20",
  "caption": "Physical Disk view",
  "type": "image",
  "file": "1!E3b9_nT-2PbZERMo0ArspA.png"
}, {
  "tag": "P",
  "text": "Inside the partition directory of a particular topic, Kafka segment file 0000–00.log, index file 0000–00.index and time index 0000–00.timeindex can be found. All the data belonging to that partition are written in an active segment as a new segment file is created when the old segment size or time limit is reached. Indexes map each offset to their message’s position in the log. Since offset is sequential binary search is applied to find a data index in log file at a particular offset.",
  "translation": "在特定主题的分区目录中，可以找到Kafka分段文件0000–00.log，索引文件0000–00.index和时间索引0000–00.timeindex。 当达到旧的段大小或时间限制时，会在创建新的段文件时将属于该分区的所有数据写入活动段中。 索引将每个偏移量映射到其消息在日志中的位置。 由于偏移量是顺序的，因此将二进制搜索应用于特定偏移量的日志文件中的数据索引。"
}, {
  "tag": "H1",
  "text": "Log Compacted Topics",
  "translation": "记录压缩主题"
}, {
  "tag": "P",
  "text": "Duplicate Keys are marked for deletion from segment file. The value here can be updated and deleted by passing a null value to a particular key.",
  "translation": "重复键标记为要从段文件中删除。 可以通过将空值传递给特定键来更新和删除此处的值。"
}, {
  "tag": "P",
  "text": "Thanks for reading! Watch this space for advanced Kafka story. Please do read my other stories as well.",
  "translation": "谢谢阅读！ 观看此空间以获取高级卡夫卡故事。 请也阅读我的其他故事。"
}, {
  "tag": "H1",
  "text": "Apache Kafka in Depth",
  "translation": "深入了解Apache Kafka"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*MLZBRP6vgDS2PQoU6jcBxw.png?q=20",
  "type": "image",
  "file": "1!MLZBRP6vgDS2PQoU6jcBxw.png"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*Jw-coW5Y9AqpiMzwZm4I3w.png?q=20",
  "type": "image",
  "file": "1!Jw-coW5Y9AqpiMzwZm4I3w.png"
}, {
  "tag": "P",
  "text": "In the era of Big Data, lots and lots of data(volume) are being produced every second(velocity) from various sources like social media, blogs that I am writing currently, e-commerce, etc., which gets stored across different platforms in different schemas(varieties). In order to perform any ETL (Extract, Transform, Load) operation, a messaging/streaming system is needed which should be asynchronous and loosely coupled i.e. data from various sources/clients like hdfs, Cassandra, RDBMS, application log file, etc. could be dumped at a single place at the same time without all the clients depending on each other. One of the solutions to the problem is Kafka — An open-source distributed streaming platform created by LinkedIn and later donated to Apache. It is written in Scala.",
  "translation": "在大数据时代，每秒（大量）的数据（量）都是从各种来源（如社交媒体，我当前正在写的博客，电子商务等）中生成的，这些数据存储在不同的平台上 在不同的模式（品种）。 为了执行任何ETL（提取，转换，加载）操作，需要一个消息传递/流传输系统，该系统应该是异步且松散耦合的，即来自各种源/客户端（如hdfs，Cassandra，RDBMS，应用程序日志文件等）的数据都可以。 可以在同一时间将其转储到一个位置，而无需所有客户端相互依赖。 解决该问题的方法之一是Kafka —一种由LinkedIn创建的开源分布式流平台，后来捐赠给了Apache。 它是用Scala编写的。"
}, {
  "tag": "H1",
  "text": "Terminology",
  "translation": "术语"
}, {
  "tag": "P",
  "text": "Messages: It is basically a key-value pair contains useful data/record in the value section.",
  "translation": "消息：基本上是一个键值对，在值部分包含有用的数据/记录。"
}, {
  "tag": "P",
  "text": "Topic: For multi-tenancy, multiple topics can be created which is just a feed name to which messages are published and subscribed.",
  "translation": "主题：对于多租户，可以创建多个主题，这些主题只是将消息发布和订阅到的源名称。"
}, {
  "tag": "P",
  "text": "Offset: Messages are stored in a sequential form similar to commit log and a sequential id is provided to each message starting from 0.",
  "translation": "偏移量：消息以类似于提交日志的顺序形式存储，并且从0开始为每个消息提供顺序ID。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*A6a1_dtaOTg-xFT3SoviUg.png?q=20",
  "caption": "Messages in a queue",
  "type": "image",
  "file": "1!A6a1_dtaOTg-xFT3SoviUg.png"
}, {
  "tag": "P",
  "text": "Broker: Kafka cluster consists of brokers which are just nodes in the cluster hosting stateless server maintained by a zookeeper. Since there is no master-slave concept here, all brokers are peers. Let's understand zookeeper first before proceeding further.",
  "translation": "代理：Kafka群集由代理组成，代理只是托管由Zookeeper维护的无状态服务器的群集中的节点。 由于这里没有主从概念，因此所有代理都是对等的。 在继续进行之前，让我们先了解一下动物园管理员。"
}, {
  "tag": "H1",
  "text": "What is Zookeeper and Why is it needed in Kafka Cluster?",
  "translation": "什么是Zookeeper？为什么在Kafka Cluster中需要它？"
}, {
  "tag": "P",
  "text": "Zookeeper is a system for distributed cluster management. It is a distributed key-value store. It is highly-optimized for reads but writes are slower. It consists of an odd number of znodes known as an ensemble. In Kafka, it is needed for:",
  "translation": "Zookeeper是用于分布式集群管理的系统。 它是一个分布式键值存储。 对其进行了高度优化，但写入速度较慢。 它由称为集合的奇数个znode组成。 在卡夫卡，它需要用于："
}, {
  "tag": "OL",
  "texts": ["Controller Election: All the read and writes from a partition for particular topics happen through the leader of the replica. Whenever the leader goes down, a new leader is elected by the zookeeper.", "Configuration of Topics: Metadata related to a topic that whether a particular topic is sitting in the broker, how many partitions are there, etc. are stored at the zookeeper end and are continuously in-sync whenever a message is produced.", "Access Control List(ACL) of a topic is maintained at a zookeeper."],
  "translations": ["控制器选举：针对特定主题的分区中的所有读取和写入均通过副本的领导者进行。 每当领导者下台时，动物园管理员都会选举新的领导者。", "主题配置：与某个主题相关的元数据，即某个特定主题是否位于代理中，存在多少分区等，存储在Zookeeper端，并在产生消息时不断保持同步。", "主题的访问控制列表（ACL）由动物园管理员维护。"]
}, {
  "tag": "H1",
  "text": "Why Kafka?",
  "translation": "为什么选择卡夫卡？"
}, {
  "tag": "P",
  "text": "Some of the key features of Kafka, which is a challenge for conventional messaging system makes it more popular:",
  "translation": "Kafka的一些关键功能（这是传统消息传递系统所面临的挑战）使其更加流行："
}, {
  "tag": "OL",
  "texts": ["High Throughput: Throughput stands for the number of messages in a second (rate of messages) that can be processed. As we can partition the topic which can spread across different brokers, we can achieve thousands of reads and writes per second.", "Distributed: A distributed system is one which is split into multiple running machines, all of which work together in a cluster to appear as one single node to the end-user. Kafka is distributed as it stores, read and write the data on several nodes called a broker which along with Zookeeper collectively creates an ecosystem known as Kafka Cluster.", "Persistence: The message queue is maintained completely on a disk rather than keeping it in memory and several copies/replica called as ISR (in-sync replica)of the same data can be stored across different nodes. Hence, there is no chance of data loss due to failover scenarios and makes it durable.", "Scalability: Any system can be scaled horizontally or vertically. Vertical scalability means adding more resources like CPU, Memory to the same nodes and incurs a high operational cost. Horizontal scalability can be achieved by simply adding a few more nodes in the cluster which increases the capacity demands. Kafka scales horizontally means we can add a new nodes/broker in the cluster whenever we run out of capacity/space.", "Fault-Tolerant: If we have n topics each having m partitions then all n*m partitions will be replicated on q brokers if we set the replication factor to q. Hence, making it tolerant as a factor of q-1 i.e. we can afford the failure of q-1 broker nodes. Replication factor should always be less than or equal to the number of brokers as violating this condition will end up having two copies of the same replica on a single broker which doesn’t make sense."],
  "translations": ["高吞吐量：吞吐量表示每秒可以处理的消息数（消息速率）。 由于我们可以划分可分布在不同代理之间的主题，因此每秒可以实现数千次读写。", "分布式：分布式系统是指被分成多个运行中的计算机的系统，所有这些计算机在一个群集中一起工作，以显示为最终用户的单个节点。 当Kafka在几个称为“代理”的节点上存储，读取和写入数据时，就进行了分布。该代理与Zookeeper共同创建了一个称为“ Kafka群集”的生态系统。", "持久性：消息队列完全保留在磁盘上，而不是保留在内存中，并且可以在不同节点上存储相同数据的多个副本/副本，称为ISR（同步副本）。 因此，由于故障转移方案而导致数据丢失的可能性不大，并且使其具有持久性。", "可扩展性：任何系统都可以水平或垂直扩展。 垂直可伸缩性意味着将更多资源（如CPU，内存）添加到相同的节点，并导致较高的运营成本。 通过简单地在集群中添加更多的节点就可以实现水平可伸缩性，从而增加了容量需求。 Kafka水平扩展意味着每当我们用完容量/空间不足时，就可以在集群中添加新的节点/代理。", "容错：如果我们有n个主题，每个主题有m个分区，那么如果我们将复制因子设置为q，则所有n * m个分区都将在q个代理上被复制。 因此，将其容忍为q-1的因子，即我们可以承受q-1代理节点的故障。 复制因子应始终小于或等于代理数量，因为违反此条件将最终在单个代理上具有同一副本的两个副本，这是没有意义的。"]
}, {
  "tag": "P",
  "text": "Note: Kafka guarantees at-least-once delivery by default and allows the user to implement at most once delivery by disabling retries on the producer and committing its offset prior to processing a batch of messages.",
  "translation": "注意：默认情况下，Kafka保证至少一次传送，并允许用户通过在生产者上禁用重试并在处理一批消息之前提交其偏移量来最多实施一次传送。"
}, {
  "tag": "H2",
  "text": "Hands-on:",
  "translation": "动手："
}, {
  "tag": "P",
  "text": "Kafka scala library can be downloaded from here: http://kafka.apache.org/downloads which contains the zookeeper as well.",
  "translation": "您可以从以下网址下载Kafka scala库：http://kafka.apache.org/downloads，其中也包含zookeeper。"
}, {
  "tag": "P",
  "text": "After extraction of zip, let’s say HOME_DIR = kafka_2.12–2.3.0/",
  "translation": "提取zip后，假设HOME_DIR = kafka_2.12–2.3.0 /"
}, {
  "tag": "P",
  "text": "First of all, we need to turn the zookeeper up by providing the dataDir name and port in HOME_DIR/config/zookeeper.properties.",
  "translation": "首先，我们需要通过在HOME_DIR / config / zookeeper.properties中提供dataDir名称和端口来打开Zookeeper。"
}, {
  "tag": "P",
  "text": "./bin/zookeeper-server-start.sh ../config/zookeeper.properties",
  "translation": "./bin/zookeeper-server-start.sh ../config/zookeeper.properties"
}, {
  "tag": "P",
  "text": "Zookeeper will be up and running on 2181 port by default. Now, we need to start Kafka server. the script is located in the bin folder. Server related config can be made in HOME_DIR/config/server.properties. Let’s make",
  "translation": "默认情况下，Zookeeper将在2181端口上启动并运行。 现在，我们需要启动Kafka服务器。 该脚本位于bin文件夹中。 与服务器相关的配置可以在HOME_DIR / config / server.properties中进行。 让我们做"
}, {
  "tag": "P",
  "text": "broker.id=101 listeners=PLAINTEXT:localhost:9091logldirs=some_log_dir/kafka-logs-1",
  "translation": "broker.id = 101侦听器= PLAINTEXT：localhost：9091logldirs = some_log_dir / kafka-logs-1"
}, {
  "tag": "P",
  "text": "./bin/kafka-servert-start.sh ../config/server.properties",
  "translation": "./bin/kafka-servert-start.sh ../config/server.properties"
}, {
  "tag": "P",
  "text": "This will start a Kafka server at port 9091. Cheatsheets:",
  "translation": "这将在端口9091处启动Kafka服务器。"
}, {
  "tag": "PRE",
  "text": "Creating a topic:bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 partitions 1 --topic topic1Describing a particular topic:bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic1Deleting a topic:bin/kafka-topics.sh --delete --zookeeper localhost:2181 --topic topic1",
  "translation": "创建主题：bin / kafka-topics.sh-创建--zookeeper本地主机：2181-复制因子3个分区1 --topic topic1描述特定主题：bin / kafka-topics.sh --describe --zookeeper localhost ：2181 --topic topic1删除主题：bin / kafka-topics.sh-删除--zookeeper localhost：2181 --topic topic1"
}, {
  "tag": "P",
  "text": "Note: Partition once set to a particular value cannot be decreased, it can only be increased.",
  "translation": "注意：一旦设置为特定值，分区将无法减少，只能增加。"
}, {
  "tag": "H1",
  "text": "How Producer writes a message?",
  "translation": "生产者如何编写消息？"
}, {
  "tag": "P",
  "text": "The Producer first fetches the Metadata of the topic in order to know which broker needs to be updated with the message. Metadata is also stored at the brokers and is in continuous sync with the zookeeper because the zookeeper nodes are generally very less as compared to the no. of brokers. So, many producers would like to connect to the zookeeper to access the metadata and the performance degrades. Now, once the producer gets the metadata about the topic and partition, it writes the message in logs of the leader broker node and followers (ISR) copy it.",
  "translation": "生产者首先获取主题的元数据，以便知道需要使用消息更新哪个代理。 元数据也存储在代理中，并且与动物园管理员保持连续同步，因为动物园管理员节点通常比No节点少得多。 经纪人。 因此，许多生产者想连接到Zookeeper来访问元数据，并且性能降低。 现在，一旦生产者获取了有关主题和分区的元数据，它将消息写入领导者经纪人节点的日志中，而关注者（ISR）将其复制。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/56/0*TtVpBYmH-uuZsqSG.png?q=20",
  "caption": "Getting metadata info — source",
  "type": "image",
  "file": "0!TtVpBYmH-uuZsqSG.png"
}, {
  "tag": "P",
  "text": "This writes operation can be either synchronous [i.e. the status about the message acknowledgment is returned to the producer only when the followers also copy that message in their log] or asynchronous [i.e. only the leader is updated with the new message, status is sent to the producer]. Retention Period: Messages on a disk can be persisted for a particular duration of time known as retention period, after that period automatic purging of the old messages will happen and will no longer be available for consumption. By default, it’s set to 7 days.",
  "translation": "此写入操作可以是同步的[即 仅当关注者还在其日志中复制该消息时]或异步[即， 只用新消息更新领导者，状态发送给生产者]。 保留期限：磁盘上的消息可以保留特定的持续时间，称为保留期限，在此期限之后，将自动清除旧消息，并且不再可供使用。 默认情况下，设置为7天。"
}, {
  "tag": "P",
  "text": "The message can be written to a topic in three Strategies:",
  "translation": "可以通过三种策略将消息写到主题："
}, {
  "tag": "P",
  "text": "a. send(key, value, topic, partition): specifically providing the partition in which writes needs to happen. This is not encouraged to use as it may create an imbalance in partition size.",
  "translation": "一种。 send（key，value，topic，partition）：专门提供需要进行写操作的分区。 不建议使用此方法，因为它可能会导致分区大小不平衡。"
}, {
  "tag": "P",
  "text": "b. send(key, value, topic): Here, default HashPartitioner is used to determine the partition in which the message will be written by finding the hash of key and taking mod with no. of partition for that topic. Our own custom Partitioner can also be written.",
  "translation": "b。 send（key，value，topic）：在这里，默认的HashPartitioner用于确定要写入消息的分区，方法是查找key的哈希并以no取mod。 该主题的分区。 也可以编写我们自己的自定义分区程序。"
}, {
  "tag": "P",
  "text": "c. send(key=null, value, topic): In this case, the message is stored in all the partition in a round-robin fashion.",
  "translation": "C。 send（key = null，value，topic）：在这种情况下，消息以循环方式存储在所有分区中。"
}, {
  "tag": "H2",
  "text": "Hands-on:",
  "translation": "动手："
}, {
  "tag": "PRE",
  "text": "Creating a console producer:bin/kafka-console-producer.sh --broker-list localhost:9091 --topic topic1Changing retention period for a topic to 10sec:bin/kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name topic1 --add-config retention.ms=10000",
  "translation": "创建控制台生产者：bin / kafka-console-producer.sh --broker-list localhost：9091 --topic topic1将主题的保留期更改为10sec：bin / kafka-configs.sh --zookeeper localhost：2181 --alter --entity-type主题--entity-name topic1 --add-config保留.ms = 10000"
}, {
  "tag": "P",
  "text": "A producer can send the messages in the form of a batch to improve efficiency. Once a batch reaches a particular size limit, it is dumped to the queue once. However, the offset will be sequential only for all the individual message and are deflated at the consumer end before passing it to the consumer API.",
  "translation": "生产者可以批量发送消息以提高效率。 批次达到特定大小限制后，将其一次转储到队列中。 但是，偏移量仅对所有单个消息而言是连续的，并且在将其传递给使用者API之前在使用者端进行缩小。"
}, {
  "tag": "H2",
  "text": "Producer API :",
  "translation": "生产者API："
}, {
  "tag": "P",
  "text": "A Kafka producer is an application that can act as a source of data in a Kafka cluster. A producer can publish messages to one or more Kafka topics using the API provided by the Kafka jar files/dependencies. A properties object containing the configuration on storing the message needs to set before sending the message. Main classes ProducerRecord, KafkaProducer, Callback.",
  "translation": "Kafka生产者是可以充当Kafka集群中数据源的应用程序。 生产者可以使用Kafka jar文件/依赖项提供的API将消息发布到一个或多个Kafka主题。 在发送消息之前，需要设置包含存储消息配置的属性对象。 主类ProducerRecord，KafkaProducer，Callback。"
}, {
  "tag": "P",
  "text": "Sample APIs:",
  "translation": "示例API："
}, {
  "tag": "FIGURE",
  "type": "code",
  "raw": "https://gist.github.com/ercsonusharma/bb890e59f28172a53395627ea5b16814/raw/17f13e3a7e1b3ae82512798933649b2d08a5bb62/SampleProducer.java",
  "code": "Producer<String, String> producer = new KafkaProducer <>(createProperties());\n\n        ProducerRecord<String, String> record = new\n            ProducerRecord<>(TOPIC_NAME, \"SyncKey\", \"SyncMessage\");\n\n        try {\n            RecordMetadata recordMetadata = producer.send(record).get();\n            System.out.println(\"Message is sent to Partition no \" +\n                recordMetadata.partition() + \" and offset \" + recordMetadata.offset());\n\n        } catch (Exception e) {\n            System.out.println(\"Exception--> \"+ e);\n        } finally{\n            producer.close();\n        }\n\n    }\n    private static Properties createProperties() {\n        Properties properties = new Properties();\n        properties.put(\"bootstrap.servers\", \"localhost:9091,localhost:9092\");\n        properties.put(\"key.serializer\",\"org.apache.kafka.common.serialization.StringSerializer\");\n        properties.put(\"value.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\");\n        return properties;\n\n    }"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Sonu Sharma的文章《Apache Kafka in Depth》，参考：https://medium.com/@sonusharma.mnnit/apache-kafka-in-depth-49aae1e844be)",
  "translation": "（本文翻译自Sonu Sharma的文章《 Apache Kafka in Depth》，参考：https：//medium.com/@sonusharma.mnnit/apache-kafka-in-depth-49aae1e844be）"
}]