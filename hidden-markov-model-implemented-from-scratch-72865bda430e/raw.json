[{
  "tag": "P",
  "text": "Originally published at https://zerowithdot.com.",
  "translation": "最初发布在https://zerowithdot.com。"
}, {
  "tag": "H1",
  "text": "Hidden Markov Model — Implemented from scratch",
  "translation": "隐藏的马尔可夫模型-从头开始实施"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*_9sOLluzCXbiePjjRzeJ4w.jpeg?q=20",
  "caption": "A short pause in the summer heat. Portugal, 2019.",
  "type": "image",
  "file": "1!_9sOLluzCXbiePjjRzeJ4w.jpeg"
}, {
  "tag": "H1",
  "text": "Introduction",
  "translation": "介绍"
}, {
  "tag": "P",
  "text": "The Internet is full of good articles that explain the theory behind the Hidden Markov Model (HMM) well (e.g. 1, 2, 3 and 4). However, many of these works contain a fair amount of rather advanced mathematical equations. While equations are necessary if one wants to explain the theory, we decided to take it to the next level and create a gentle step by step practical implementation to complement the good work of others.",
  "translation": "互联网上到处都是很好的文章，很好地解释了隐马尔可夫模型（HMM）的理论（例如1、2、3和4）。 但是，其中许多作品都包含相当数量的相当高级的数学方程式。 虽然方程式对于解释这一理论是必要的，但我们决定将其推向新的高度，并逐步创建一个温和的实际实施方案，以补充他人的出色工作。"
}, {
  "tag": "P",
  "text": "In this short series of two articles, we will focus on translating all of the complicated mathematics into code. Our starting point is the document written by Mark Stamp. We will use this paper to define our code (this article) and then use a somewhat peculiar example of “Morning Insanity” to demonstrate its performance in practice.",
  "translation": "在这两篇文章的简短系列中，我们将重点介绍将所有复杂的数学转换为代码。 我们的起点是Mark Stamp编写的文档。 我们将使用本文来定义我们的代码（本文），然后使用一个有点奇怪的“早晨精神错乱”示例来演示其在实践中的性能。"
}, {
  "tag": "H2",
  "text": "Notation",
  "translation": "符号"
}, {
  "tag": "P",
  "text": "Before we begin, let’s revisit the notation we will be using. By the way, don’t worry if some of that is unclear to you. We will hold your hand.",
  "translation": "在开始之前，我们先回顾一下将要使用的符号。 顺便说一句，如果您不清楚其中的某些内容，请不要担心。 我们会牵着你的手。"
}, {
  "tag": "UL",
  "texts": ["T - length of the observation sequence.", "N - number of latent (hidden) states.", "M - number of observables.", "Q = {q₀, q₁, …} - hidden states.", "V = {0, 1, …, M — 1} - set of possible observations.", "A - state transition matrix.", "B - emission probability matrix.", "π- initial state probability distribution.", "O - observation sequence.", "X = (x₀, x₁, …), x_t ∈ Q - hidden state sequence."],
  "translations": ["T-观察序列的长度。", "N-潜在（隐藏）状态的数量。", "M-可观察数。", "Q = {q₀，q₁，…}-隐藏状态。", "V = {0，1，…，M_1}-可能的观测值集合。", "-状态转换矩阵。", "B-发射概率矩阵。", "π-初始状态概率分布。", "O-观察序列。", "X =（x₀，x₁，…），x_t∈Q-隐藏状态序列。"]
}, {
  "tag": "P",
  "text": "Having that set defined, we can calculate the probability of any state and observation using the matrices:",
  "translation": "定义该集合后，我们可以使用矩阵来计算任何状态和观察的概率："
}, {
  "tag": "UL",
  "texts": ["A = {a_ij} — begin an transition matrix.", "B = {b_j(k)} — being an emission matrix."],
  "translations": ["A = {a_ij}-开始一个转换矩阵。", "B = {b_j（k）} —是一个发射矩阵。"]
}, {
  "tag": "P",
  "text": "The probabilities associated with transition and observation (emission) are:",
  "translation": "与过渡和观测（发射）相关的概率为："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*T0Zhx76AK8Wbxi9E0sIvPQ.png?q=20",
  "type": "image",
  "file": "1!T0Zhx76AK8Wbxi9E0sIvPQ.png"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*lZqmmKNImg_M9DEqbrcFgQ.png?q=20",
  "type": "image",
  "file": "1!lZqmmKNImg_M9DEqbrcFgQ.png"
}, {
  "tag": "P",
  "text": "The model is therefore defined as a collection:",
  "translation": "因此，该模型被定义为一个集合："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*ZpWRw7mtNyISVlcJw-QzRg.png?q=20",
  "type": "image",
  "file": "1!ZpWRw7mtNyISVlcJw-QzRg.png"
}, {
  "tag": "H1",
  "text": "Fundamental definitions",
  "translation": "基本定义"
}, {
  "tag": "P",
  "text": "Since HMM is based on probability vectors and matrices, let’s first define objects that will represent the fundamental concepts. To be useful, the objects must reflect on certain properties. For example, all elements of a probability vector must be numbers 0 ≤ x ≤ 1 and they must sum up to 1. Therefore, let’s design the objects the way they will inherently safeguard the mathematical properties.",
  "translation": "由于HMM基于概率矢量和矩阵，因此我们首先定义代表基本概念的对象。 为了有用，对象必须反映某些属性。 例如，概率向量的所有元素必须为数字0≤x≤1，并且它们的总和必须为1。因此，让我们设计对象以固有的方式保护数学属性的方式。"
}, {
  "tag": "PRE",
  "text": "import numpy as npimport pandas as pdclass ProbabilityVector:    def __init__(self, probabilities: dict):        states = probabilities.keys()        probs  = probabilities.values()                assert len(states) == len(probs),             \"The probabilities must match the states.\"        assert len(states) == len(set(states)),             \"The states must be unique.\"        assert abs(sum(probs) - 1.0) < 1e-12,             \"Probabilities must sum up to 1.\"        assert len(list(filter(lambda x: 0 <= x <= 1, probs))) == len(probs), \\            \"Probabilities must be numbers from [0, 1] interval.\"                self.states = sorted(probabilities)        self.values = np.array(list(map(lambda x:             probabilities[x], self.states))).reshape(1, -1)            @classmethod    def initialize(cls, states: list):        size = len(states)        rand = np.random.rand(size) / (size**2) + 1 / size        rand /= rand.sum(axis=0)        return cls(dict(zip(states, rand)))        @classmethod    def from_numpy(cls, array: np.ndarray, state: list):        return cls(dict(zip(states, list(array))))    @property    def dict(self):        return {k:v for k, v in zip(self.states, list(self.values.flatten()))}    @property    def df(self):        return pd.DataFrame(self.values, columns=self.states, index=['probability'])    def __repr__(self):        return \"P({}) = {}.\".format(self.states, self.values)    def __eq__(self, other):        if not isinstance(other, ProbabilityVector):            raise NotImplementedError        if (self.states == other.states) and (self.values == other.values).all():            return True        return False    def __getitem__(self, state: str) -> float:        if state not in self.states:            raise ValueError(\"Requesting unknown probability state from vector.\")        index = self.states.index(state)        return float(self.values[0, index])    def __mul__(self, other) -> np.ndarray:        if isinstance(other, ProbabilityVector):            return self.values * other.values        elif isinstance(other, (int, float)):            return self.values * other        else:            NotImplementedError    def __rmul__(self, other) -> np.ndarray:        return self.__mul__(other)    def __matmul__(self, other) -> np.ndarray:        if isinstance(other, ProbabilityMatrix):            return self.values @ other.values    def __truediv__(self, number) -> np.ndarray:        if not isinstance(number, (int, float)):            raise NotImplementedError        x = self.values        return x / number if number != 0 else x / (number + 1e-12)    def argmax(self):        index = self.values.argmax()        return self.states[index]",
  "translation": "import numpy as npimport pandas as pdclass ProbabilityVector：def __init __（自我，概率：dict）：状态=概率.keys（）概率=概率.values（）断言len（状态）== len（概率），“概率必须匹配美国。” assert len（states）== len（set（states）），“状态必须是唯一的。”断言abs（sum（probs）-1.0）<1e-12，“概率总和为1”。断言len（list（filter（lambda x：0 <= x <= 1，probs）））== len（probs），\\“概率必须是[0，1]间隔内的数字。” self.states = sorted（概率）self.values = np.array（list（map（lambda x：probabilities [x]，self.states）））。reshape（1，-1）@classmethod def initialize（cls，states） ：list）：size = len（states）rand = np.random.rand（size）/（size ** 2）+1 / size rand / = rand.sum（axis = 0）return cls（dict（zip（states） ，rand）））@classmethod def from_numpy（cls，array：np.ndarray，state：list）：return cls（dict（zip（states，list（array）））））@property def dict（self）：return {k ：v for k，zip中的v，v（zip（self.states，list（self.values.flatten（））））} @属性def df（self）：返回pd.DataFrame（self.values，column = self.states，index = ['probability']）def __repr __（self）：返回“ P（{}）= {}。”。format（self.states，self.values）def __eq __（self，other）：如果不是isinstance（其他，ProbabilityVector） ）：如果（self.states == other.states）和（self.values == other.values）.all（）则引发NotImplementedError：返回True返回False def def __ getitem __（self，state：str）-> float：如果状态不在self.states：提高ValueError（“从向量请求未知概率状态。”）index = self.states.index（state）返回float （self.values [0，index]）def __mul __（self，other）-> np.ndarray：如果isinstance（other，ProbabilityVector）：返回self.values *其他值elif isinstance（other，（int，float）） ：返回self.values *其他：NotImplementedError def __rmul __（self，other）-> np.ndarray：返回self .__ mul __（other）def __matmul __（self，other）-> np.ndarray：if isinstance（other，ProbabilityMatrix） ：返回self.values @ other.values def __truediv __（self，number）-> np.ndarray：如果不是isinstance（number，（int，float））：引发NotImplementedError x = self.values返回x /数字，如果number！= 0 else x /（数字+ 1e-12）def arg max（self）：index = self.values.argmax（）返回self.states [index]"
}, {
  "tag": "P",
  "text": "The most natural way to initialize this object is to use a dictionary as it associates values with unique keys. Dictionaries, unfortunately, do not provide any assertion mechanisms that put any constraints on the values. Consequently, we build our custom ProbabilityVector object to ensure that our values behave correctly. Most importantly, we enforce the following:",
  "translation": "初始化此对象的最自然的方法是使用字典，因为它将值与唯一键相关联。 不幸的是，字典没有提供任何对值施加约束的断言机制。 因此，我们构建了自定义的ProbabilityVector对象以确保我们的值正确运行。 最重要的是，我们强制执行以下操作："
}, {
  "tag": "UL",
  "texts": ["The number of values must equal the number of the keys (names of our states). Although this is not a problem when initializing the object from a dictionary, we will use other ways later.", "All names of the states must be unique (the same arguments apply).", "The probabilities must sum up to 1 (up to a certain tolerance).", "All probabilities must be 0 ≤ p ≤ 1."],
  "translations": ["值的数量必须等于键的数量（我们各州的名称）。 尽管从字典初始化对象时这不是问题，但是稍后我们将使用其他方式。", "状态的所有名称必须唯一（适用相同的参数）。", "概率总和必须为1（达到一定的容差）。", "所有概率必须为0≤p≤1。"]
}, {
  "tag": "P",
  "text": "Having ensured that, we also provide two alternative ways to instantiate ProbabilityVector objects (decorated with @classmethod).",
  "translation": "确保这一点之后，我们还提供了两种方法来实例化ProbabilityVector对象（使用@classmethod装饰）。"
}, {
  "tag": "OL",
  "texts": ["We instantiate the objects randomly — it will be useful when training.", "We use ready-made numpy arrays and use values therein, and only providing the names for the states."],
  "translations": ["我们随机实例化对象-训练时将很有用。", "我们使用现成的numpy数组并在其中使用值，并且仅提供状态的名称。"]
}, {
  "tag": "P",
  "text": "For convenience and debugging, we provide two additional methods for requesting the values. Decorated with, they return the content of the PV object as a dictionary or a pandas dataframe.",
  "translation": "为了方便和调试，我们提供了两种其他方法来请求值。 装饰后，它们以字典或熊猫数据框的形式返回PV对象的内容。"
}, {
  "tag": "P",
  "text": "The PV objects need to satisfy the following mathematical operations (for the purpose of constructing of HMM):",
  "translation": "PV对象需要满足以下数学运算（出于构造HMM的目的）："
}, {
  "tag": "OL",
  "texts": ["comparison (__eq__) - to know if any two PV's are equal,", "element-wise multiplication of two PV’s or multiplication with a scalar (__mul__ and __rmul__).", "dot product (__matmul__) - to perform vector-matrix multiplication", "division by number (__truediv__),", "argmax to find for which state the probability is the highest.", "__getitem__ to enable selecting value by the key."],
  "translations": ["比较（__eq__）-要知道两个PV是否相等，", "两个PV的按元素乘法或标量（__mul__和__rmul__）的乘法。", "点积（__matmul__）-执行向量矩阵乘法", "除以数字（__truediv__），", "argmax查找概率最高的状态。", "__getitem__以启用通过键选择值。"]
}, {
  "tag": "P",
  "text": "Note that when e.g. multiplying a PV with a scalar, the returned structure is a resulting numpy array, not another PV. This is because multiplying by anything other than 1 would violate the integrity of the PV itself.",
  "translation": "请注意，例如 将PV与标量相乘，返回的结构是结果numpy数组，而不是另一个PV。 这是因为乘以1以外的任何值都会违反PV本身的完整性。"
}, {
  "tag": "P",
  "text": "Internally, the values are stored as a numpy array of size (1 × N).",
  "translation": "在内部，这些值存储为大小为（1×N）的numpy数组。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*3kJ1kVSOYyCpHNp2cpeq-g.png?q=20",
  "type": "image",
  "file": "1!3kJ1kVSOYyCpHNp2cpeq-g.png"
}, {
  "tag": "H2",
  "text": "Example",
  "translation": "例"
}, {
  "tag": "PRE",
  "text": "a1 = ProbabilityVector({'rain': 0.7, 'sun': 0.3})a2 = ProbabilityVector({'sun': 0.1, 'rain': 0.9})print(a1.df)print(a2.df)print(\"Comparison:\", a1 == a2)print(\"Element-wise multiplication:\", a1 * a2)print(\"Argmax:\", a1.argmax())print(\"Getitem:\", a1['rain'])# OUTPUT>>>              rain  sun    probability   0.7  0.3                 rain  sun    probability   0.9  0.1>>> Comparison: False>>> Element-wise multiplication: [[0.63 0.03]]>>> Argmax: rain>>> Getitem: 0.7",
  "translation": "a1 = ProbabilityVector（{'rain'：0.7，'sun'：0.3}）a2 = ProbabilityVector（{'sun'：0.1，'rain'：0.9}）print（a1.df）print（a2.df）print（ “比较：”，a1 == a2）print（“逐元素乘法：”，a1 * a2）print（“ Argmax：”，a1.argmax（））print（“ Getitem：”，a1 ['rain'] ）＃输出>>>阴阳几率0.7 0.3阴阳几率0.9 0.1 >>>比较：False >>>逐元素乘法：[[0.63 0.03]] >>> Argmax：阴雨>>> Getitem：0.7"
}, {
  "tag": "H2",
  "text": "Probability Matrix",
  "translation": "概率矩阵"
}, {
  "tag": "P",
  "text": "Another object is a Probability Matrix, which is a core part of the HMM definition. Formally, the A and B matrices must be row-stochastic, meaning that the values of every row must sum up to 1. We can, therefore, define our PM by stacking several PV's, which we have constructed in a way to guarantee this constraint.",
  "translation": "另一个对象是概率矩阵，它是HMM定义的核心部分。 形式上，A和B矩阵必须是行随机的，这意味着每行的值必须总和为1。因此，我们可以通过堆叠几个PV来定义PM，我们通过构造PV来保证这种约束 。"
}, {
  "tag": "PRE",
  "text": "class ProbabilityMatrix:    def __init__(self, prob_vec_dict: dict):                assert len(prob_vec_dict) > 1, \\            \"The numebr of input probability vector must be greater than one.\"        assert len(set([str(x.states) for x in prob_vec_dict.values()])) == 1, \\            \"All internal states of all the vectors must be indentical.\"        assert len(prob_vec_dict.keys()) == len(set(prob_vec_dict.keys())), \\            \"All observables must be unique.\"        self.states      = sorted(prob_vec_dict)        self.observables = prob_vec_dict[self.states[0]].states        self.values      = np.stack([prob_vec_dict[x].values \\                           for x in self.states]).squeeze()     @classmethod    def initialize(cls, states: list, observables: list):        size = len(states)        rand = np.random.rand(size, len(observables)) \\             / (size**2) + 1 / size        rand /= rand.sum(axis=1).reshape(-1, 1)        aggr = [dict(zip(observables, rand[i, :])) for i in range(len(states))]        pvec = [ProbabilityVector(x) for x in aggr]        return cls(dict(zip(states, pvec)))    @classmethod    def from_numpy(cls, array:                   np.ndarray,                   states: list,                   observables: list):        p_vecs = [ProbabilityVector(dict(zip(observables, x))) \\                  for x in array]        return cls(dict(zip(states, p_vecs)))    @property    def dict(self):        return self.df.to_dict()    @property    def df(self):        return pd.DataFrame(self.values,                columns=self.observables, index=self.states)    def __repr__(self):        return \"PM {} states: {} -> obs: {}.\".format(            self.values.shape, self.states, self.observables)    def __getitem__(self, observable: str) -> np.ndarray:        if observable not in self.observables:            raise ValueError(\"Requesting unknown probability observable from the matrix.\")        index = self.observables.index(observable)        return self.values[:, index].reshape(-1, 1)",
  "translation": "class ProbabilityMatrix：def __init __（self，prob_vec_dict：dict）：断言len（prob_vec_dict）> 1，\\“输入概率向量的数值必须大于1。”断言len（set（[prob_vec_dict.values（）中x的str（x.states）））== 1，\\“所有向量的所有内部状态必须相同。”断言len（prob_vec_dict.keys（））== len（set（prob_vec_dict.keys（））），\\“所有可观察对象必须唯一。” self.states =已排序（prob_vec_dict）self.observables = prob_vec_dict [self.states [0]]。states self.values = np.stack（[prob_vec_dict [x] .values \\ for self.states中的x））。squeeze（ ）@classmethod def initialize（cls，states：list，observables：list）：size = len（states）rand = np.random.rand（size，len（observables））\\ /（size ** 2）+ 1 / size rand / = rand.sum（axis = 1）.reshape（-1，1）aggr = [dict（zip（zip（observables，rand [i，：]）））for i in range（len（states））] pvec = [ aggr中x的ProbabilityVector（x）返回cls（dict（zip（states，pvec）））@classmethod def from_numpy（cls，array：np.ndarray，States：list，observables：list）：p_vecs = [ProbabilityVector（dict （zip（observables，x）））\\数组中的x]返回cls（dict（zip（states，p_vecs）））@property def dict（self）：返回self.df.to_dict（）@property def df（self）：返回pd.DataFrame（self.values，columns = self.observables，index = self.states）def __repr __（self）：返回“ PM {}状态：{}-> obs：{}。” .format（self.values.shape，self.states，self.observables）def __getitem __（self，observable：str）-> np.ndarray：如果在self.observables中不可见，则引发ValueError（“请求从矩阵。“）index = self.observables.index（observable）返回self.values [:, index] .reshape（-1，1）"
}, {
  "tag": "P",
  "text": "Here, the way we instantiate PM’s is by supplying a dictionary of PV’s to the constructor of the class. By doing this, we not only ensure that every row of PM is stochastic, but also supply the names for every observable.",
  "translation": "在这里，我们实例化PM的方法是向类的构造函数提供PV的字典。 这样，我们不仅可以确保PM的每一行都是随机的，而且还可以为每个可观测的名称提供名称。"
}, {
  "tag": "P",
  "text": "Our PM can, therefore, give an array of coefficients for any observable. Mathematically, the PM is a matrix:",
  "translation": "因此，我们的PM可以为任何可观察到的系数给出一个数组。 从数学上讲，PM是一个矩阵："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*YywRl3rWWpWJrlMdyKTlFw.png?q=20",
  "type": "image",
  "file": "1!YywRl3rWWpWJrlMdyKTlFw.png"
}, {
  "tag": "P",
  "text": "The other methods are implemented in similar way to PV.",
  "translation": "其他方法的实现方式与PV相似。"
}, {
  "tag": "H2",
  "text": "Example",
  "translation": "例"
}, {
  "tag": "PRE",
  "text": "a1 = ProbabilityVector({'rain': 0.7, 'sun': 0.3})a2 = ProbabilityVector({'rain': 0.6, 'sun': 0.4})A  = ProbabilityMatrix({'hot': a1, 'cold': a2})print(A)print(A.df)>>> PM (2, 2) states: ['cold', 'hot'] -> obs: ['rain', 'sun'].>>>      rain  sun   cold   0.6  0.4   hot    0.7  0.3b1 = ProbabilityVector({'0S': 0.1, '1M': 0.4, '2L': 0.5})b2 = ProbabilityVector({'0S': 0.7, '1M': 0.2, '2L': 0.1})B =  ProbabilityMatrix({'0H': b1, '1C': b2})print(B)print(B.df)>>> PM (2, 3) states: ['0H', '1C'] -> obs: ['0S', '1M', '2L'].>>>       0S   1M   2L     0H  0.1  0.4  0.5     1C  0.7  0.2  0.1P = ProbabilityMatrix.initialize(list('abcd'), list('xyz'))print('Dot product:', a1 @ A)print('Initialization:', P)print(P.df)>>> Dot product: [[0.63 0.37]]>>> Initialization: PM (4, 3)     states: ['a', 'b', 'c', 'd'] -> obs: ['x', 'y', 'z'].>>>          x         y         z   a  0.323803  0.327106  0.349091   b  0.318166  0.326356  0.355478   c  0.311833  0.347983  0.340185   d  0.337223  0.316850  0.345927",
  "translation": "a1 = ProbabilityVector（{'rain'：0.7，'sun'：0.3}）a2 = ProbabilityVector（{'rain'：0.6，'sun'：0.4}）A = ProbabilityMatrix（{'hot'：a1，'cold' ：a2}）print（A）print（A.df）>>> PM（2，2）状态：['冷'，'热']->肥胖：['雨'，'太阳']。>> >雨日冷0.6 0.4热0.7 0.3b1 = ProbabilityVector（{'0S'：0.1，'1M'：0.4，'2L'：0.5}）b2 = ProbabilityVector（{'0S'：0.7，'1M'：0.2， '2L'：0.1}）B =概率矩阵（{'0H'：b1，'1C'：b2}）print（B）print（B.df）>>> PM（2，3）状态：['0H' ，'1C']-> obs：['0S'，'1M'，'2L']。>>> 0S 1M 2L 0H 0.1 0.4 0.5 1C 0.7 0.2 0.1P = ProbabilityMatrix.initialize（list（'abcd'）， list（'xyz'））print（'Dot product：'，a1 @ A）print（'Initialization：'，P）print（P.df）>>>点积：[[0.63 0.37]] >>>初始化 ：PM（4，3）状态：['a'，'b'，'c'，'d']-> obs：['x'，'y'，'z']。>>> xyza 0.323803 0.327106 0.349091 b 0.318166 0.326356 0.355478 c 0.311833 0.347983 0.340185 d 0.337223 0.316850 0.345927"
}, {
  "tag": "H1",
  "text": "Implementing Hidden Markov Chain",
  "translation": "实施隐马尔可夫链"
}, {
  "tag": "P",
  "text": "Before we proceed with calculating the score, let’s use our PV and PM definitions to implement the Hidden Markov Chain.",
  "translation": "在继续计算分数之前，让我们使用PV和PM定义实施隐马尔可夫链。"
}, {
  "tag": "P",
  "text": "Again, we will do so as a class, calling it HiddenMarkovChain. It will collate at A, B and π. Later on, we will implement more methods that are applicable to this class.",
  "translation": "同样，我们将作为一个类，将其称为HiddenMarkovChain。 它将在A，B和π处进行整理。 稍后，我们将实现更多适用于此类的方法。"
}, {
  "tag": "H2",
  "text": "Computing score",
  "translation": "计算分数"
}, {
  "tag": "P",
  "text": "Computing the score means to find what is the probability of a particular chain of observations O given our (known) model λ = (A, B, π). In other words, we are interested in finding p(O|λ).",
  "translation": "计算分数意味着在给定我们的（已知）模型λ=（A，B，π）的情况下，发现特定观察链O的概率是多少。 换句话说，我们对找到p（O |λ）很感兴趣。"
}, {
  "tag": "P",
  "text": "We can find p(O|λ) by marginalizing all possible chains of the hidden variables X, where X = {x₀, x₁, …}:",
  "translation": "我们可以通过对隐藏变量X的所有可能链进行边际化来找到p（O |λ），其中X = {x₀，x₁，…}："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*iwv-JeejXCGA83nXsjQMbQ.png?q=20",
  "type": "image",
  "file": "1!iwv-JeejXCGA83nXsjQMbQ.png"
}, {
  "tag": "P",
  "text": "Since p(O|X, λ) = ∏ b(O) (the product of all probabilities related to the observables) and p(X|λ)=π ∏ a (the product of all probabilities of transitioning from x at t to x at t + 1, the probability we are looking for (the score) is:",
  "translation": "由于p（O | X，λ）= ∏ b（O）（与可观察物相关的所有概率的乘积）和p（X |λ）=π∏ a（从x在t处转变为所有概率的乘积 x在t + 1处，我们寻找的概率（分数）为："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*64rWhxvZgBI_aa6poPm0gw.png?q=20",
  "type": "image",
  "file": "1!64rWhxvZgBI_aa6poPm0gw.png"
}, {
  "tag": "P",
  "text": "This is a naive way of computing of the score, since we need to calculate the probability for every possible chain X. Either way, let’s implement it in python:",
  "translation": "这是一种简单的分数计算方式，因为我们需要计算每个可能的链X的概率。无论哪种方式，让我们在python中实现它："
}, {
  "tag": "PRE",
  "text": "from itertools import productfrom functools import reduceclass HiddenMarkovChain:    def __init__(self, T, E, pi):        self.T = T  # transmission matrix A        self.E = E  # emission matrix B        self.pi = pi        self.states = pi.states        self.observables = E.observables        def __repr__(self):        return \"HML states: {} -> observables: {}.\".format(            len(self.states), len(self.observables))        @classmethod    def initialize(cls, states: list, observables: list):        T = ProbabilityMatrix.initialize(states, states)        E = ProbabilityMatrix.initialize(states, observables)        pi = ProbabilityVector.initialize(states)        return cls(T, E, pi)        def _create_all_chains(self, chain_length):        return list(product(*(self.states,) * chain_length))        def score(self, observations: list) -> float:        def mul(x, y): return x * y                score = 0        all_chains = self._create_all_chains(len(observations))        for idx, chain in enumerate(all_chains):            expanded_chain = list(zip(chain, [self.T.states[0]] + list(chain)))            expanded_obser = list(zip(observations, chain))                        p_observations = list(map(lambda x: self.E.df.loc[x[1], x[0]], expanded_obser))            p_hidden_state = list(map(lambda x: self.T.df.loc[x[1], x[0]], expanded_chain))            p_hidden_state[0] = self.pi[chain[0]]                        score += reduce(mul, p_observations) * reduce(mul, p_hidden_state)        return score",
  "translation": "从itertools导入产品从functools导入reduceclass HiddenMarkovChain：def __init __（self，T，E，pi）：self.T = T＃传输矩阵A self.E = E＃发射矩阵B self.pi = pi self.states = pi。状态self.observables = E.observables def __repr __（self）：返回“ HML状态：{}-> observables：{}。”。format（len（self.states），len（self.observables））@classmethod def初始化（cls，states：list，observables：list）：T = ProbabilityMatrix.initialize（states，states）E = ProbabilityMatrix.initialize（states，observables）pi = ProbabilityVector.initialize（states）返回cls（T，E，pi）def _create_all_chains（self，chain_length）：return list（product（*（self.states，）* chain_length））def score（self，observations：list）-> float：def mul（x，y）：return x * y score = 0个all_chains = self._create_all_chains（len（observations））for idx，枚举（all_chains）中的链：expanded_chain = list（zip（链，[self.T.states [0]] + list（链）））Expanded_obser =列表（zip（observations，chain））p_observations = list（map（lambda x：self.E.df.loc [x [1]，x [0]]，expanded_obser））p_hidden_​​state = list（map（lambda x：self.E.df.loc [x [1]，x [0] ]，Expanded_chain））p_hidden_​​state [0] = self.pi [chain [0]]得分+ = reduce（mul，p_observations）* reduce（mul，p_hidden_​​state）返回得分"
}, {
  "tag": "H2",
  "text": "Example",
  "translation": "例"
}, {
  "tag": "PRE",
  "text": "a1 = ProbabilityVector({'1H': 0.7, '2C': 0.3})a2 = ProbabilityVector({'1H': 0.4, '2C': 0.6})b1 = ProbabilityVector({'1S': 0.1, '2M': 0.4, '3L': 0.5})b2 = ProbabilityVector({'1S': 0.7, '2M': 0.2, '3L': 0.1})A = ProbabilityMatrix({'1H': a1, '2C': a2})B = ProbabilityMatrix({'1H': b1, '2C': b2})pi = ProbabilityVector({'1H': 0.6, '2C': 0.4})hmc = HiddenMarkovChain(A, B, pi)observations = ['1S', '2M', '3L', '2M', '1S']print(\"Score for {} is {:f}.\".format(observations, hmc.score(observations)))>>> Score for ['1S', '2M', '3L', '2M', '1S'] is 0.003482.",
  "translation": "a1 = ProbabilityVector（{'1H'：0.7，'2C'：0.3}）a2 = ProbabilityVector（{'1H'：0.4，'2C'：0.6}）b1 = ProbabilityVector（{'1S'：0.1，'2M' ：0.4，'3L'：0.5}）b2 = ProbabilityVector（{'1S'：0.7，'2M'：0.2，'3L'：0.1}）A = ProbabilityMatrix（{'1H'：a1，'2C'：a2 }）B = ProbabilityMatrix（{'1H'：b1，'2C'：b2}）pi = ProbabilityVector（{'1H'：0.6，'2C'：0.4}）hmc = HiddenMarkovChain（A，B，pi）观测值= ['1S'，'2M'，'3L'，'2M'，'1S'] print（“ {}的得分为{：f}。”。format（observations，hmc.score（observations）））>> > ['1S'，'2M'，'3L'，'2M'，'1S']的得分是0.003482。"
}, {
  "tag": "P",
  "text": "If our implementation is correct, then all score values for all possible observation chains, for a given model should add up to one. Namely:",
  "translation": "如果我们的实现是正确的，则对于给定模型，所有可能的观察链的所有得分值应加起来为1。 即："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*_hQL9-7vqRtmakk0epwH7Q.png?q=20",
  "type": "image",
  "file": "1!_hQL9-7vqRtmakk0epwH7Q.png"
}, {
  "tag": "PRE",
  "text": "all_possible_observations = {'1S', '2M', '3L'}chain_length = 3  # any int > 0all_observation_chains = list(product(*(all_possible_observations,) * chain_length))all_possible_scores = list(map(lambda obs: hmc.score(obs), all_observation_chains))print(\"All possible scores added: {}.\".format(sum(all_possible_scores)))>>> All possible scores added: 1.0.",
  "translation": "all_possible_observations = {'1S'，'2M'，'3L'} chain_length = 3＃任何整数> 0all_observation_chains = list（product（*（all_possible_observations，）* chain_length））all_possible_scores = list（map（lambda obs：hmc.score（ obs），all_observation_chains））print（“添加的所有可能分数：{}。”。format（sum（all_possible_scores）））>>>添加的所有可能分数：1.0。"
}, {
  "tag": "P",
  "text": "Indeed.",
  "translation": "确实。"
}, {
  "tag": "H1",
  "text": "Score with forward-pass",
  "translation": "前进得分"
}, {
  "tag": "P",
  "text": "Computing the score the way we did above is kind of naive. In order to find the number for a particular observation chain O, we have to compute the score for all possible latent variable sequences X. That requires 2TN^T multiplications, which even for small numbers takes time.",
  "translation": "以上面的方式计算分数很幼稚。 为了找到特定观察链O的数目，我们必须计算所有可能的潜在变量序列X的分数。这需要2TN ^ T乘法，即使对于较小的数目也需要时间。"
}, {
  "tag": "P",
  "text": "Another way to do it is to calculate partial observations of a sequence up to time t.",
  "translation": "另一种方法是计算直到时间t的序列的部分观测值。"
}, {
  "tag": "P",
  "text": "For and i ∈ {0, 1, …, N-1} and t ∈ {0, 1, …, T-1} :",
  "translation": "对于和i∈{0，1，…，N-1}和t∈{0，1，…，T-1}："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*mXqvaUwFkNfqpjorU27NDA.png?q=20",
  "type": "image",
  "file": "1!mXqvaUwFkNfqpjorU27NDA.png"
}, {
  "tag": "P",
  "text": "Consequently,",
  "translation": "所以，"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*JSGl-T_Z8bDkrmtyJwVAww.png?q=20",
  "type": "image",
  "file": "1!JSGl-T_Z8bDkrmtyJwVAww.png"
}, {
  "tag": "P",
  "text": "and",
  "translation": "和"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*iMouiFNUBISWwqnNbbvmVg.png?q=20",
  "type": "image",
  "file": "1!iMouiFNUBISWwqnNbbvmVg.png"
}, {
  "tag": "P",
  "text": "Then",
  "translation": "然后"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*jIMu6YqeNDqNeEAnkB-pjw.png?q=20",
  "type": "image",
  "file": "1!jIMu6YqeNDqNeEAnkB-pjw.png"
}, {
  "tag": "P",
  "text": "Note that α_t is a vector of length N. The sum of the product α a can, in fact, be written as a dot product. Therefore:",
  "translation": "注意，α_t是长度为N的向量。乘积αa的总和实际上可以写为点积。 因此："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*KwVaGXQwHbcqipX-CV3l8w.png?q=20",
  "type": "image",
  "file": "1!KwVaGXQwHbcqipX-CV3l8w.png"
}, {
  "tag": "P",
  "text": "where by the star, we denote an element-wise multiplication.",
  "translation": "在星号上，我们表示逐元素乘法。"
}, {
  "tag": "P",
  "text": "With this implementation, we reduce the number of multiplication to N²T and can take advantage of vectorization.",
  "translation": "通过此实现，我们可以将乘法次数减少到N²T，并可以利用矢量化的优势。"
}, {
  "tag": "PRE",
  "text": "class HiddenMarkovChain_FP(HiddenMarkovChain):    def _alphas(self, observations: list) -> np.ndarray:        alphas = np.zeros((len(observations), len(self.states)))        alphas[0, :] = self.pi.values * self.E[observations[0]].T        for t in range(1, len(observations)):            alphas[t, :] = (alphas[t - 1, :].reshape(1, -1)                          @ self.T.values) * self.E[observations[t]].T        return alphas        def score(self, observations: list) -> float:        alphas = self._alphas(observations)        return float(alphas[-1].sum())",
  "translation": "HiddenMarkovChain_FP（HiddenMarkovChain）类：def _alphas（self，观察值：list）-> np.ndarray：alphas = np.zeros（（（len（observations），len（self.states））））alphas [0，：] = self。 pi.values * self.E [observations [0]]。T在范围（1，len（observations））中的t：alphas [t，：] =（alphas [t-1，：]。reshape（1，- 1）@ self.T.values）* self.E [observations [t]]。T返回alphas def分数（self，observations：列表）-> float：alphas = self._alphas（observations）返回float（alphas [- 1] .sum（））"
}, {
  "tag": "H2",
  "text": "Example",
  "translation": "例"
}, {
  "tag": "PRE",
  "text": "hmc_fp = HiddenMarkovChain_FP(A, B, pi)observations = ['1S', '2M', '3L', '2M', '1S']print(\"Score for {} is {:f}.\".format(observations, hmc_fp.score(observations)))>>> All possible scores added: 1.0.",
  "translation": "hmc_fp = HiddenMarkovChain_FP（A，B，pi）观测值= ['1S'，'2M'，'3L'，'2M'，'1S'] print（“ {}的得分为{：f}。”。format（ 观测值，hmc_fp.score（observations）））>>>所有可能的得分增加：1.0。"
}, {
  "tag": "P",
  "text": "…yup.",
  "translation": "…对。"
}, {
  "tag": "H1",
  "text": "Simulation and convergence",
  "translation": "仿真与融合"
}, {
  "tag": "P",
  "text": "Let’s test one more thing. Basically, let’s take our λ = (A, B, π) and use it to generate a sequence of random observables, starting from some initial state probability π.",
  "translation": "让我们再测试一件事。 基本上，让我们以λ=（A，B，π）为基础，并使用它从某个初始状态概率π开始生成一系列随机可观测量。"
}, {
  "tag": "P",
  "text": "If the desired length T is “large enough”, we would expect that the system to converge on a sequence that, on average, gives the same number of events as we would expect from A and B matrices directly. In other words, the transition and the emission matrices “decide”, with a certain probability, what the next state will be and what observation we will get, for every step, respectively. Therefore, what may initially look like random events, on average should reflect the coefficients of the matrices themselves. Let’s check that as well.",
  "translation": "如果期望的长度T是“足够大”，则我们期望系统收敛于一个序列，该序列平均提供与直接从A和B矩阵期望的事件数量相同的事件数。 换句话说，过渡矩阵和发射矩阵以一定的概率“决定”每一步的下一状态是什么以及观察到的结果。 因此，最初看起来像随机事件的平均水平应该反映矩阵本身的系数。 我们也检查一下。"
}, {
  "tag": "PRE",
  "text": "class HiddenMarkovChain_Simulation(HiddenMarkovChain):    def run(self, length: int) -> (list, list):        assert length >= 0, \"The chain needs to be a non-negative number.\"        s_history = [0] * (length + 1)        o_history = [0] * (length + 1)                prb = self.pi.values        obs = prb @ self.E.values        s_history[0] = np.random.choice(self.states, p=prb.flatten())        o_history[0] = np.random.choice(self.observables, p=obs.flatten())                for t in range(1, length + 1):            prb = prb @ self.T.values            obs = prb @ self.E.values            s_history[t] = np.random.choice(self.states, p=prb.flatten())            o_history[t] = np.random.choice(self.observables, p=obs.flatten())                return o_history, s_history",
  "translation": "“类HiddenMarkovChain_Simulation（HiddenMarkovChain）：def运行（自已，长度：int）->（列表，列表）：断言长度> = 0，“链必须是非负数。” s_history = [0] *（长度+ 1）o_history = [0] *（长度+ 1）prb = self.pi.values obs = prb @ self.E.values s_history [0] = np.random.choice（self .states，p = prb.flatten（））o_history [0] = np.random.choice（self.observables，p = obs.flatten（））for range in（1，length + 1）：prb = prb @ self.T.values obs = prb @ self.E.values s_history [t] = np.random.choice（self.states，p = prb.flatten（））o_history [t] = np.random.choice（self。 可观察的，p = obs.flatten（））返回o_history，s_history"
}, {
  "tag": "H2",
  "text": "Example",
  "translation": "例"
}, {
  "tag": "PRE",
  "text": "hmc_s = HiddenMarkovChain_Simulation(A, B, pi)observation_hist, states_hist = hmc_s.run(100)  # length = 100stats = pd.DataFrame({    'observations': observation_hist,    'states': states_hist}).applymap(lambda x: int(x[0])).plot()",
  "translation": "hmc_s = HiddenMarkovChain_Simulation（A，B，pi）observation_hist，States_hist = hmc_s.run（100）＃length = 100stats = pd.DataFrame（{'observations'：observation_hist，'states'：states_hist}）。applymap（lambda x：int （x [0]））。plot（）"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/0*lUKNRqnxALUv-5LT.png?q=20",
  "caption": "Figure 1. An example of a Markov process. The states and the observable sequences are shown.",
  "type": "image",
  "file": "0!lUKNRqnxALUv-5LT.png"
}, {
  "tag": "H1",
  "text": "Latent states",
  "translation": "潜在状态"
}, {
  "tag": "P",
  "text": "The state matrix A is given by the following coefficients:",
  "translation": "状态矩阵A由以下系数给出："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*Km-BqZiWjKNAZYKa79I3mQ.png?q=20",
  "type": "image",
  "file": "1!Km-BqZiWjKNAZYKa79I3mQ.png"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*H80JBfAN1E0rF1pzNVei9g.png?q=20",
  "type": "image",
  "file": "1!H80JBfAN1E0rF1pzNVei9g.png"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*pAq0blp3_g2THZ3cspEGUw.png?q=20",
  "type": "image",
  "file": "1!pAq0blp3_g2THZ3cspEGUw.png"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*V__J0lbI7d_Xvqpk6ZOEKA.png?q=20",
  "type": "image",
  "file": "1!V__J0lbI7d_Xvqpk6ZOEKA.png"
}, {
  "tag": "P",
  "text": "Consequently, the probability of “being” in the state “1H” at t+1, regardless of the previous state, is equal to:",
  "translation": "因此，与先前的状态无关，在t + 1时处于“ 1H”状态的“存在”概率为："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*x5-UHueLGK-_NjaITlPMjw.png?q=20",
  "type": "image",
  "file": "1!x5-UHueLGK-_NjaITlPMjw.png"
}, {
  "tag": "P",
  "text": "If we assume that the prior probabilities of being at some state at are totally random, then p(1H) = 1 and p(2C) = 0.9, which after renormalizing give 0.55 and 0.45, respectively.",
  "translation": "如果我们假设处于某个状态的先验概率是完全随机的，则p（1H）= 1和p（2C）= 0.9，这在重新归一化后分别为0.55和0.45。"
}, {
  "tag": "P",
  "text": "If we count the number of occurrences of each state and divide it by the number of elements in our sequence, we would get closer and closer to these number as the length of the sequence grows.",
  "translation": "如果我们计算每个状态的出现次数并将其除以序列中元素的数量，则随着序列长度的增加，我们将越来越接近这些状态。"
}, {
  "tag": "H2",
  "text": "Example",
  "translation": "例"
}, {
  "tag": "PRE",
  "text": "hmc_s = HiddenMarkovChain_Simulation(A, B, pi)stats = {}for length in np.logspace(1, 5, 40).astype(int):    observation_hist, states_hist = hmc_s.run(length)    stats[length] = pd.DataFrame({        'observations': observation_hist,        'states': states_hist}).applymap(lambda x: int(x[0]))S = np.array(list(map(lambda x:         x['states'].value_counts().to_numpy() / len(x), stats.values())))plt.semilogx(np.logspace(1, 5, 40).astype(int), S)plt.xlabel('Chain length T')plt.ylabel('Probability')plt.title('Converging probabilities.')plt.legend(['1H', '2C'])plt.show()"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/0*KhO5MgDJ7SWClJ_f.png?q=20",
  "caption": "Figure 2. Convergence of the probabilities against the length of the chain.",
  "type": "image",
  "file": "0!KhO5MgDJ7SWClJ_f.png"
}, {
  "tag": "P",
  "text": "Let’s take our HiddenMarkovChain class to the next level and supplement it with more methods. The methods will help us to discover the most probable sequence of hidden variables behind the observation sequence."
}, {
  "tag": "H1",
  "text": "Expanding the class",
  "translation": "扩大班级"
}, {
  "tag": "P",
  "text": "We have defined α to be the probability of partial observation of the sequence up to time .",
  "translation": "我们将α定义为部分观测到时间序列的概率。"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*iK3qr3oB96cpDrkAZEjZJw.png?q=20",
  "type": "image",
  "file": "1!iK3qr3oB96cpDrkAZEjZJw.png"
}, {
  "tag": "P",
  "text": "Now, let’s define the “opposite” probability. Namely, the probability of observing the sequence from T - 1down to t.",
  "translation": "现在，让我们定义“相反”的可能性。 即，观察从T-1向下到t的序列的概率。"
}, {
  "tag": "P",
  "text": "For t= 0, 1, …, T-1 and i=0, 1, …, N-1, we define:"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*ZZjiWEflYiIy-yGtfJUlwQ.png?q=20",
  "type": "image",
  "file": "1!ZZjiWEflYiIy-yGtfJUlwQ.png"
}, {
  "tag": "P",
  "text": "c`1As before, we can β(i) calculate recursively:",
  "translation": "c`1和以前一样，我们可以递归计算β（i）："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*-NirISd6itC4Wy_NB0LmDg.png?q=20",
  "type": "image",
  "file": "1!-NirISd6itC4Wy_NB0LmDg.png"
}, {
  "tag": "P",
  "text": "Then for t ≠ T-1:",
  "translation": "然后对于t≠T-1："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*T_s2sGP8q_quWP-zJXeHjg.png?q=20",
  "type": "image",
  "file": "1!T_s2sGP8q_quWP-zJXeHjg.png"
}, {
  "tag": "P",
  "text": "which in vectorized form, will be:",
  "translation": "矢量化形式为："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*k04Zy1_oAKq-6byfNXsLXQ.png?q=20",
  "type": "image",
  "file": "1!k04Zy1_oAKq-6byfNXsLXQ.png"
}, {
  "tag": "P",
  "text": "Finally, we also define a new quantity γ to indicate the state q_i at time t, for which the probability (calculated forwards and backwards) is the maximum:",
  "translation": "最后，我们还定义了一个新的量γ来指示时间t处的状态q_i，对于该状态，概率（向前和向后计算）的可能性最大："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*4GqFamyukrXgC5NHNmYg6g.png?q=20",
  "type": "image",
  "file": "1!4GqFamyukrXgC5NHNmYg6g.png"
}, {
  "tag": "P",
  "text": "Consequently, for any step t = 0, 1, …, T-1, the state of the maximum likelihood can be found using:"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*IocLUrCOE0HiqgeBk62hHg.png?q=20",
  "type": "image",
  "file": "1!IocLUrCOE0HiqgeBk62hHg.png"
}, {
  "tag": "PRE",
  "text": "class HiddenMarkovChain_Uncover(HiddenMarkovChain_Simulation):    def _alphas(self, observations: list) -> np.ndarray:        alphas = np.zeros((len(observations), len(self.states)))        alphas[0, :] = self.pi.values * self.E[observations[0]].T        for t in range(1, len(observations)):            alphas[t, :] = (alphas[t - 1, :].reshape(1, -1) @ self.T.values) \\                         * self.E[observations[t]].T        return alphas        def _betas(self, observations: list) -> np.ndarray:        betas = np.zeros((len(observations), len(self.states)))        betas[-1, :] = 1        for t in range(len(observations) - 2, -1, -1):            betas[t, :] = (self.T.values @ (self.E[observations[t + 1]] \\                        * betas[t + 1, :].reshape(-1, 1))).reshape(1, -1)        return betas        def uncover(self, observations: list) -> list:        alphas = self._alphas(observations)        betas = self._betas(observations)        maxargs = (alphas * betas).argmax(axis=1)        return list(map(lambda x: self.states[x], maxargs))"
}, {
  "tag": "H1",
  "text": "Validation",
  "translation": "验证方式"
}, {
  "tag": "P",
  "text": "To validate, let’s generate some observable sequence O. For that, we can use our model’s .run method. Then, we will use the.uncover method to find the most likely latent variable sequence."
}, {
  "tag": "H2",
  "text": "Example",
  "translation": "例"
}, {
  "tag": "PRE",
  "text": "np.random.seed(42)a1 = ProbabilityVector({'1H': 0.7, '2C': 0.3})a2 = ProbabilityVector({'1H': 0.4, '2C': 0.6})b1 = ProbabilityVector({'1S': 0.1, '2M': 0.4, '3L': 0.5}) b2 = ProbabilityVector({'1S': 0.7, '2M': 0.2, '3L': 0.1})A  = ProbabilityMatrix({'1H': a1, '2C': a2})B  = ProbabilityMatrix({'1H': b1, '2C': b2})pi = ProbabilityVector({'1H': 0.6, '2C': 0.4})hmc = HiddenMarkovChain_Uncover(A, B, pi)observed_sequence, latent_sequence = hmc.run(5)uncovered_sequence = hmc.uncover(observed_sequence)|                    | 0   | 1   | 2   | 3   | 4   | 5   ||:------------------:|:----|:----|:----|:----|:----|:----|| observed sequence  | 3L  | 3M  | 1S  | 3L  | 3L  | 3L  || latent sequence    | 1H  | 2C  | 1H  | 1H  | 2C  | 1H  || uncovered sequence | 1H  | 1H  | 2C  | 1H  | 1H  | 1H  |",
  "translation": "np.random.seed（42）a1 = ProbabilityVector（{'1H'：0.7，'2C'：0.3}）a2 = ProbabilityVector（{'1H'：0.4，'2C'：0.6}）b1 = ProbabilityVector（{' 1S'：0.1，'2M'：0.4，'3L'：0.5}）b2 = ProbabilityVector（{'1S'：0.7，'2M'：0.2，'3L'：0.1}）A = ProbabilityMatrix（{'1H' ：a1，'2C'：a2}）B = ProbabilityMatrix（{'1H'：b1，'2C'：b2}）pi = ProbabilityVector（{'1H'：0.6，'2C'：0.4}）hmc = HiddenMarkovChain_Uncover（ A，B，pi）observed_sequence = hmc.run（5）uncovered_sequence = hmc.uncover（observed_sequence）| | 0 | 1 | 2 | 3 | 4 | 5 ||：------------------：|：---- |：---- |：---- |：---- |：- --- |：---- || 观察序列| 3升| 3M | 1S | 3升| 3升| 3升|| 潜在序列| 1小时| 2C | 1小时| 1小时| 2C | 1H || 揭露的序列| 1小时| 1小时| 2C | 1小时| 1小时| 1小时|"
}, {
  "tag": "P",
  "text": "As we can see, the most likely latent state chain (according to the algorithm) is not the same as the one that actually caused the observations. This is to be expected. After all, each observation sequence can only be manifested with certain probability, dependent on the latent sequence.",
  "translation": "如我们所见，最可能的潜在状态链（根据算法）与实际引起观察的链不同。 这是预料之中的。 毕竟，取决于潜在序列，每个观察序列只能以一定概率出现。"
}, {
  "tag": "P",
  "text": "The code below, evaluates the likelihood of different latent sequences resulting in our observation sequence.",
  "translation": "下面的代码评估产生我们的观察序列的不同潜在序列的可能性。"
}, {
  "tag": "PRE",
  "text": "all_possible_states = {'1H', '2C'}chain_length = 6  # any int > 0all_states_chains = list(product(*(all_possible_states,) * chain_length))df = pd.DataFrame(all_states_chains)dfp = pd.DataFrame()for i in range(chain_length):    dfp['p' + str(i)] = df.apply(lambda x:         hmc.E.df.loc[x[i], observed_sequence[i]], axis=1)scores = dfp.sum(axis=1).sort_values(ascending=False)df = df.iloc[scores.index]df['score'] = scoresdf.head(10).reset_index()|    index | 0   | 1   | 2   | 3   | 4   | 5   |   score ||:--------:|:----|:----|:----|:----|:----|:----|--------:||        8 | 1H  | 1H  | 2C  | 1H  | 1H  | 1H  |     3.1 ||       24 | 1H  | 2C  | 2C  | 1H  | 1H  | 1H  |     2.9 ||       40 | 2C  | 1H  | 2C  | 1H  | 1H  | 1H  |     2.7 ||       12 | 1H  | 1H  | 2C  | 2C  | 1H  | 1H  |     2.7 ||       10 | 1H  | 1H  | 2C  | 1H  | 2C  | 1H  |     2.7 ||        9 | 1H  | 1H  | 2C  | 1H  | 1H  | 2C  |     2.7 ||       25 | 1H  | 2C  | 2C  | 1H  | 1H  | 2C  |     2.5 ||        0 | 1H  | 1H  | 1H  | 1H  | 1H  | 1H  |     2.5 ||       26 | 1H  | 2C  | 2C  | 1H  | 2C  | 1H  |     2.5 ||       28 | 1H  | 2C  | 2C  | 2C  | 1H  | 1H  |     2.5 |",
  "translation": "all_possible_states = {'1H'，'2C'} chain_length = 6＃任何整数> 0all_states_chains = list（product（*（all_possible_states，）* chain_length））df = pd.DataFrame（all_states_chains）dfp = pd.DataFrame（）在范围内（链长）：dfp ['p'+ str（i）] = df.apply（lambda x：hmc.E.df.loc [x [i]，observed_sequence [i]]，axis = 1）得分= dfp.sum（axis = 1）.sort_values（升序= False）df = df.iloc [scores.index] df ['score'] = scoresdf.head（10）.reset_index（）|索引| 0 | 1 | 2 | 3 | 4 | 5 |得分||：--------：|：---- |：---- |：---- |：---- |：---- |：-------- | --------：|| 8 | 1小时| 1小时| 2C | 1小时| 1小时| 1小时| 3.1 || 24 | 1小时| 2C | 2C | 1小时| 1小时| 1小时| 2.9 || 40 | 2C | 1小时| 2C | 1小时| 1小时| 1小时| 2.7 || 12 | 1小时| 1小时| 2C | 2C | 1小时| 1小时| 2.7 || 10 | 1小时| 1小时| 2C | 1小时| 2C | 1小时| 2.7 || 9 | 1小时| 1小时| 2C | 1小时| 1小时| 2C | 2.7 || 25 | 1小时| 2C | 2C | 1小时| 1小时| 2C | 2.5 || 0 | 1小时| 1小时| 1小时| 1小时| 1小时| 1小时| 2.5 || 26 | 1小时| 2C | 2C | 1小时| 2C | 1小时| 2.5 || 28 | 1小时| 2C | 2C | 2C | 1小时| 1小时| 2.5 |"
}, {
  "tag": "P",
  "text": "The result above shows the sorted table of the latent sequences, given the observation sequence. The actual latent sequence (the one that caused the observations) places itself on the 35th position (we counted index from zero).",
  "translation": "上面的结果显示了给定观察序列的潜在序列的排序表。 实际的潜在序列（引起观测的序列）位于第35位（我们从零开始计算索引）。"
}, {
  "tag": "PRE",
  "text": "dfc = df.copy().reset_index()for i in range(chain_length):    dfc = dfc[dfc[i] == latent_sequence[i]]    dfc|   index | 0   | 1   | 2   | 3   | 4   | 5   |   score ||:-------:|:----|:----|:----|:----|:----|:----|--------:||      18 | 1H  | 2C  | 1H  | 1H  | 2C  | 1H  |     1.9 |",
  "translation": "dfc = df.copy（）。reset_index（），用于范围（链长）中的i：dfc = dfc [dfc [i] == latent_sequence [i]] dfc | 索引| 0 | 1 | 2 | 3 | 4 | 5 | 得分||：-------：|：---- |：---- |：---- |：---- |：---- | :: ||- -------：|| 18 | 1小时| 2C | 1小时| 1小时| 2C | 1小时| 1.9 |"
}, {
  "tag": "H1",
  "text": "Training the model",
  "translation": "训练模型"
}, {
  "tag": "P",
  "text": "The time has come to show the training procedure. Formally, we are interested in finding λ = (A, B, π) such that given a desired observation sequence O, our model λ would give the best fit.",
  "translation": "现在是展示培训程序的时候了。 形式上，我们有兴趣寻找λ=（A，B，π），以便在给定所需的观察序列O的情况下，我们的模型λ将提供最佳拟合。"
}, {
  "tag": "H1",
  "text": "Expanding the class",
  "translation": "扩大班级"
}, {
  "tag": "P",
  "text": "Here, our starting point will be the HiddenMarkovModel_Uncover that we have defined earlier. We will add new methods to train it.",
  "translation": "在这里，我们的起点将是我们先前定义的HiddenMarkovModel_Uncover。 我们将添加新的方法进行培训。"
}, {
  "tag": "P",
  "text": "Knowing our latent states Q and possible observation states O, we automatically know the sizes of the matrices A and B, hence N and M. However, we need to determine a and b and π.",
  "translation": "知道了我们的潜在状态Q和可能的观察状态O，我们就自动知道了矩阵A和B的大小，因此也知道了N和M。但是，我们需要确定a和b和π。"
}, {
  "tag": "P",
  "text": "For t = 0, 1, …, T-2 and i, j =0, 1, …, N -1, we define “di-gammas”:",
  "translation": "对于t = 0、1，...，T-2和i，j = 0、1，...，N -1，我们定义“ di-gammas”："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*TteOdsgD8yOAosXJNepdZg.png?q=20",
  "type": "image",
  "file": "1!TteOdsgD8yOAosXJNepdZg.png"
}, {
  "tag": "P",
  "text": "γ(i, j) is the probability of transitioning for q at t to t + 1. Writing it in terms of α, β, A, B we have:",
  "translation": "γ（i，j）是q在t到t + 1跃迁的概率。用α，β，A，B表示："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*QaSTMHFVLfMZXiwMtuMhgg.png?q=20",
  "type": "image",
  "file": "1!QaSTMHFVLfMZXiwMtuMhgg.png"
}, {
  "tag": "P",
  "text": "Now, thinking in terms of implementation, we want to avoid looping over i, j and t at the same time, as it’s gonna be deadly slow. Fortunately, we can vectorize the equation:",
  "translation": "现在，从实现的角度考虑，我们希望避免同时循环遍历i，j和t，因为这会非常致命。 幸运的是，我们可以向量化方程："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*CAsQqrG0qraQYKQXanF06Q.png?q=20",
  "type": "image",
  "file": "1!CAsQqrG0qraQYKQXanF06Q.png"
}, {
  "tag": "P",
  "text": "Having the equation for γ(i, j), we can calculate",
  "translation": "有了γ（i，j）的方程，我们可以计算"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*xsfpOsxdOoTLgU7ji_MVTQ.png?q=20",
  "type": "image",
  "file": "1!xsfpOsxdOoTLgU7ji_MVTQ.png"
}, {
  "tag": "P",
  "text": "To find λ = (A, B, π), we do",
  "translation": "为了找到λ=（A，B，π），我们做"
}, {
  "tag": "P",
  "text": "For i = 0, 1, …, N-1:",
  "translation": "对于i = 0、1，...，N-1："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*K0j0--FQysUHgOIfG_XSqw.png?q=20",
  "type": "image",
  "file": "1!K0j0--FQysUHgOIfG_XSqw.png"
}, {
  "tag": "P",
  "text": "or",
  "translation": "要么"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*Q_1_NefGd77m8rkGiIq3gA.png?q=20",
  "type": "image",
  "file": "1!Q_1_NefGd77m8rkGiIq3gA.png"
}, {
  "tag": "P",
  "text": "For i, j = 0, 1, …, N-1:"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*zkEQbIs1ItiFw5ZUO9kKcQ.png?q=20",
  "type": "image",
  "file": "1!zkEQbIs1ItiFw5ZUO9kKcQ.png"
}, {
  "tag": "P",
  "text": "For j = 0, 1, …, N-1 and k = 0, 1, …, M-1:",
  "translation": "对于j = 0、1，...，N-1和k = 0、1，...，M-1："
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/1*AHpjdfCk7dSXn9-NNUEyJA.png?q=20",
  "type": "image",
  "file": "1!AHpjdfCk7dSXn9-NNUEyJA.png"
}, {
  "tag": "PRE",
  "text": "class HiddenMarkovLayer(HiddenMarkovChain_Uncover):    def _digammas(self, observations: list) -> np.ndarray:        L, N = len(observations), len(self.states)        digammas = np.zeros((L - 1, N, N))        alphas = self._alphas(observations)        betas = self._betas(observations)        score = self.score(observations)        for t in range(L - 1):            P1 = (alphas[t, :].reshape(-1, 1) * self.T.values)            P2 = self.E[observations[t + 1]].T * betas[t + 1].reshape(1, -1)            digammas[t, :, :] = P1 * P2 / score        return digammas"
}, {
  "tag": "P",
  "text": "Having the “layer” supplemented with the ._difammas method, we should be able to perform all the necessary calculations. However, it makes sense to delegate the \"management\" of the layer to another class. In fact, the model training can be summarized as follows:",
  "translation": "通过对._difammas方法补充“层”，我们应该能够执行所有必要的计算。 但是，将层的“管理”委托给另一个类是有意义的。 实际上，模型训练可以总结如下："
}, {
  "tag": "OL",
  "texts": ["Initialize A, B and π.", "Calculate γ(i, j).", "Update the model’s A, B and π.", "We repeat the 2. and 3. until the score p(O|λ) no longer increases."],
  "translations": ["初始化A，B和π。", "计算γ（i，j）。", "更新模型的A，B和π。", "我们重复2.和3.直到得分p（O |λ）不再增加。"]
}, {
  "tag": "PRE",
  "text": "class HiddenMarkovModel:    def __init__(self, hml: HiddenMarkovLayer):        self.layer = hml        self._score_init = 0        self.score_history = []    @classmethod    def initialize(cls, states: list, observables: list):        layer = HiddenMarkovLayer.initialize(states, observables)        return cls(layer)    def update(self, observations: list) -> float:        alpha = self.layer._alphas(observations)        beta = self.layer._betas(observations)        digamma = self.layer._digammas(observations)        score = alpha[-1].sum()        gamma = alpha * beta / score         L = len(alpha)        obs_idx = [self.layer.observables.index(x) \\                  for x in observations]        capture = np.zeros((L, len(self.layer.states), len(self.layer.observables)))        for t in range(L):            capture[t, :, obs_idx[t]] = 1.0        pi = gamma[0]        T = digamma.sum(axis=0) / gamma[:-1].sum(axis=0).reshape(-1, 1)        E = (capture * gamma[:, :, np.newaxis]).sum(axis=0) / gamma.sum(axis=0).reshape(-1, 1)        self.layer.pi = ProbabilityVector.from_numpy(pi, self.layer.states)        self.layer.T = ProbabilityMatrix.from_numpy(T, self.layer.states, self.layer.states)        self.layer.E = ProbabilityMatrix.from_numpy(E, self.layer.states, self.layer.observables)                    return score    def train(self, observations: list, epochs: int, tol=None):        self._score_init = 0        self.score_history = (epochs + 1) * [0]        early_stopping = isinstance(tol, (int, float))        for epoch in range(1, epochs + 1):            score = self.update(observations)            print(\"Training... epoch = {} out of {}, score = {}.\".format(epoch, epochs, score))            if early_stopping and abs(self._score_init - score) / score < tol:                print(\"Early stopping.\")                break            self._score_init = score            self.score_history[epoch] = score",
  "translation": "类HiddenMarkovModel：def __init __（self，hml：HiddenMarkovLayer）：self.layer = hml self._score_init = 0 self.score_history = [] @classmethod def initialize（cls，状态：列表，可观察的：列表）：layer = HiddenMarkovLayer.initialize （状态，可观察值）返回cls（layer）def update（self，observations：list）-> float：alpha = self.layer._alphas（observations）beta = self.layer._betas（observations）digamma = self.layer._digammas （观测值）得分= alpha [-1] .sum（）伽玛= alpha * beta /分数L = len（alpha）obs_idx = [对于观测值中x的self.layer.observables.index（x）\\]捕获= np。范围（L）中t的零（（L，len（self.layer.states），len（self.layer.observables））））：capture [t，：，obs_idx [t]] = 1.0 pi = gamma [0 ] T = digamma.sum（axis = 0）/ gamma [：-1] .sum（axis = 0）.reshape（-1，1）E =（捕获* gamma [:,：，np.newaxis]）。总和（轴= 0） / gamma.sum（axis = 0）.reshape（-1，1）self.layer.pi = ProbabilityVector.from_numpy（pi，self.layer.states）self.layer.T = ProbabilityMatrix.from_numpy（T，self.layer .states，self.layer.states）self.layer.E = ProbabilityMatrix.from_numpy（E，self.layer.states，self.layer.observables）返回得分def train（self，观察值：列表，历元：int，tol =无）：self._score_init = 0 self.score_history =（历元+ 1）* [0] early_stopping = isinstance（tol，（int，float））对于范围（1，历元+ 1）中的历元：分数= self.update （观察）print（“培训... epoch = {}，出{}，分数= {}。”。format（epoch，epochs，score））if early_stopping and abs（self._score_init-score）/ score <tol ：print（“ Early stop。”）break self._score_init =得分self.score_history [epoch] =得分"
}, {
  "tag": "H2",
  "text": "Example",
  "translation": "例"
}, {
  "tag": "PRE",
  "text": "np.random.seed(42)observations = ['3L', '2M', '1S', '3L', '3L', '3L']states = ['1H', '2C']observables = ['1S', '2M', '3L']hml = HiddenMarkovLayer.initialize(states, observables)hmm = HiddenMarkovModel(hml)hmm.train(observations, 25)",
  "translation": "np.random.seed（42）观测值= ['3L'，'2M'，'1S'，'3L'，'3L'，'3L']状态= ['1H'，'2C'] observables = [' 1S'，'2M'，'3L'] hml = HiddenMarkovLayer.initialize（状态，可观察值）hmm = HiddenMarkovModel（hml）hmm.train（观察值，25）"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/0*UXkAcwTqcRzy3bo_.png?q=20",
  "caption": "Figure 3. Example of the score funciton during training.",
  "type": "image",
  "file": "0!UXkAcwTqcRzy3bo_.png"
}, {
  "tag": "H1",
  "text": "Verification",
  "translation": "验证"
}, {
  "tag": "P",
  "text": "Let’s look at the generated sequences. The “demanded” sequence is:",
  "translation": "让我们看一下生成的序列。 “需求”顺序为："
}, {
  "tag": "PRE",
  "text": "|    | 0   | 1   | 2   | 3   | 4   | 5   ||---:|:----|:----|:----|:----|:----|:----||  0 | 3L  | 2M  | 1S  | 3L  | 3L  | 3L  |RUNS = 100000T = 5chains = RUNS * [0]for i in range(len(chains)):    chain = hmm.layer.run(T)[0]    chains[i] = '-'.join(chain)",
  "translation": "| | 0 | 1 | 2 | 3 | 4 | 5 || ----：|：---- |：---- |：---- |：---- |：---- |：---- ||| 0 | 3升| 2M | 1S | 3升| 3升| 3L | RUNS = 100000T = 5链= RUNS * [0]对于范围内的i（len（链））：链= hmm.layer.run（T）[0]链[i] ='-'。join（链）"
}, {
  "tag": "P",
  "text": "The table below summarizes simulated runs based on 100000 attempts (see above), with the frequency of occurrence and number of matching observations.",
  "translation": "下表总结了基于100000次尝试（见上文）的模拟运行，并显示了发生频率和匹配观察值的数量。"
}, {
  "tag": "P",
  "text": "The bottom line is that if we have truly trained the model, we should see a strong tendency for it to generate us sequences that resemble the one we require. Let’s see if it happens.",
  "translation": "底线是，如果我们真正地训练了模型，我们应该看到它有很强的趋势来生成类似于我们所需序列的序列。 让我们看看它是否发生。"
}, {
  "tag": "PRE",
  "text": "df = pd.DataFrame(pd.Series(chains).value_counts(), columns=['counts']).reset_index().rename(columns={'index': 'chain'})df = pd.merge(df, df['chain'].str.split('-', expand=True), left_index=True, right_index=True)s = []for i in range(T + 1):    s.append(df.apply(lambda x: x[i] == observations[i], axis=1))df['matched'] = pd.concat(s, axis=1).sum(axis=1)df['counts'] = df['counts'] / RUNS * 100df = df.drop(columns=['chain'])df.head(30)---|---:|---------:|:----|:----|:----|:----|:----|:----|----------:||  0 |    8.907 | 3L  | 3L  | 3L  | 3L  | 3L  | 3L  |         4 ||  1 |    4.422 | 3L  | 2M  | 3L  | 3L  | 3L  | 3L  |         5 ||  2 |    4.286 | 1S  | 3L  | 3L  | 3L  | 3L  | 3L  |         3 ||  3 |    4.284 | 3L  | 3L  | 3L  | 3L  | 3L  | 2M  |         3 ||  4 |    4.278 | 3L  | 3L  | 3L  | 2M  | 3L  | 3L  |         3 ||  5 |    4.227 | 3L  | 3L  | 1S  | 3L  | 3L  | 3L  |         5 ||  6 |    4.179 | 3L  | 3L  | 3L  | 3L  | 1S  | 3L  |         3 ||  7 |    2.179 | 3L  | 2M  | 3L  | 2M  | 3L  | 3L  |         4 ||  8 |    2.173 | 3L  | 2M  | 3L  | 3L  | 1S  | 3L  |         4 ||  9 |    2.165 | 1S  | 3L  | 1S  | 3L  | 3L  | 3L  |         4 || 10 |    2.147 | 3L  | 2M  | 3L  | 3L  | 3L  | 2M  |         4 || 11 |    2.136 | 3L  | 3L  | 3L  | 2M  | 3L  | 2M  |         2 || 12 |    2.121 | 3L  | 2M  | 1S  | 3L  | 3L  | 3L  |         6 || 13 |    2.111 | 1S  | 3L  | 3L  | 2M  | 3L  | 3L  |         2 || 14 |    2.1   | 1S  | 2M  | 3L  | 3L  | 3L  | 3L  |         4 || 15 |    2.075 | 3L  | 3L  | 3L  | 2M  | 1S  | 3L  |         2 || 16 |    2.05  | 1S  | 3L  | 3L  | 3L  | 3L  | 2M  |         2 || 17 |    2.04  | 3L  | 3L  | 1S  | 3L  | 3L  | 2M  |         4 || 18 |    2.038 | 3L  | 3L  | 1S  | 2M  | 3L  | 3L  |         4 || 19 |    2.022 | 3L  | 3L  | 1S  | 3L  | 1S  | 3L  |         4 || 20 |    2.008 | 1S  | 3L  | 3L  | 3L  | 1S  | 3L  |         2 || 21 |    1.955 | 3L  | 3L  | 3L  | 3L  | 1S  | 2M  |         2 || 22 |    1.079 | 1S  | 2M  | 3L  | 2M  | 3L  | 3L  |         3 || 23 |    1.077 | 1S  | 2M  | 3L  | 3L  | 3L  | 2M  |         3 || 24 |    1.075 | 3L  | 2M  | 1S  | 2M  | 3L  | 3L  |         5 || 25 |    1.064 | 1S  | 2M  | 1S  | 3L  | 3L  | 3L  |         5 || 26 |    1.052 | 1S  | 2M  | 3L  | 3L  | 1S  | 3L  |         3 || 27 |    1.048 | 3L  | 2M  | 3L  | 2M  | 1S  | 3L  |         3 || 28 |    1.032 | 1S  | 3L  | 1S  | 2M  | 3L  | 3L  |         3 || 29 |    1.024 | 1S  | 3L  | 1S  | 3L  | 1S  | 3L  |         3 |"
}, {
  "tag": "P",
  "text": "And here are the sequences that we don’t want the model to create.",
  "translation": "这是我们不希望模型创建的序列。"
}, {
  "tag": "PRE",
  "text": "|     |   counts | 0   | 1   | 2   | 3   | 4   | 5   |   matched ||----:|---------:|:----|:----|:----|:----|:----|:----|----------:|| 266 |    0.001 | 1S  | 1S  | 3L  | 3L  | 2M  | 2M  |         1 || 267 |    0.001 | 1S  | 2M  | 2M  | 3L  | 2M  | 2M  |         2 || 268 |    0.001 | 3L  | 1S  | 1S  | 3L  | 1S  | 1S  |         3 || 269 |    0.001 | 3L  | 3L  | 3L  | 1S  | 2M  | 2M  |         1 || 270 |    0.001 | 3L  | 1S  | 3L  | 1S  | 1S  | 3L  |         2 || 271 |    0.001 | 1S  | 3L  | 2M  | 1S  | 1S  | 3L  |         1 || 272 |    0.001 | 3L  | 2M  | 2M  | 3L  | 3L  | 1S  |         4 || 273 |    0.001 | 1S  | 3L  | 3L  | 1S  | 1S  | 1S  |         0 || 274 |    0.001 | 3L  | 1S  | 2M  | 2M  | 1S  | 2M  |         1 || 275 |    0.001 | 3L  | 3L  | 2M  | 1S  | 3L  | 2M  |         2 |",
  "translation": "| | 计数| 0 | 1 | 2 | 3 | 4 | 5 | 匹配的|| ----：| ---------：|：---- |：---- |：---- |：---- | :: ---- | ：---- | ----------：|| 266 | 0.001 | 1S | 1S | 3升| 3升| 2M | 2M | 1 || 267 | 0.001 | 1S | 2M | 2M | 3升| 2M | 2M | 2 || 268 | 0.001 | 3升| 1S | 1S | 3升| 1S | 1S | 3 || 269 | 0.001 | 3升| 3升| 3升| 1S | 2M | 2M | 1 || 270 | 0.001 | 3升| 1S | 3升| 1S | 1S | 3升| 2 || 271 | 0.001 | 1S | 3升| 2M | 1S | 1S | 3升| 1 || 272 | 0.001 | 3升| 2M | 2M | 3升| 3升| 1S | 4 || 273 | 0.001 | 1S | 3升| 3升| 1S | 1S | 1S | 0 || 274 | 0.001 | 3升| 1S | 2M | 2M | 1S | 2M | 1 || 275 | 0.001 | 3升| 3升| 2M | 1S | 3升| 2M | 2 |"
}, {
  "tag": "P",
  "text": "As we can see, there is a tendency for our model to generate sequences that resemble the one we require, although the exact one (the one that matches 6/6) places itself already at the 10th position! On the other hand, according to the table, the top 10 sequences are still the ones that are somewhat similar to the one we request.",
  "translation": "正如我们所看到的，尽管确切的序列（匹配6/6的序列）已经将自己置于第10位，但我们的模型仍倾向于生成与所需序列相似的序列！ 另一方面，根据该表，前10个序列仍然与我们要求的序列有些相似。"
}, {
  "tag": "P",
  "text": "To ultimately verify the quality of our model, let’s plot the outcomes together with the frequency of occurrence and compare it against a freshly initialized model, which is supposed to give us completely random sequences — just to compare."
}, {
  "tag": "PRE",
  "text": "hml_rand = HiddenMarkovLayer.initialize(states, observables)hmm_rand = HiddenMarkovModel(hml_rand)RUNS = 100000T = 5chains_rand = RUNS * [0]for i in range(len(chains_rand)):    chain_rand = hmm_rand.layer.run(T)[0]    chains_rand[i] = '-'.join(chain_rand)df2 = pd.DataFrame(pd.Series(chains_rand).value_counts(), columns=['counts']).reset_index().rename(columns={'index': 'chain'})df2 = pd.merge(df2, df2['chain'].str.split('-', expand=True), left_index=True, right_index=True)s = []for i in range(T + 1):    s.append(df2.apply(lambda x: x[i] == observations[i], axis=1))df2['matched'] = pd.concat(s, axis=1).sum(axis=1)df2['counts'] = df2['counts'] / RUNS * 100df2 = df2.drop(columns=['chain'])fig, ax = plt.subplots(1, 1, figsize=(14, 6))ax.plot(df['matched'], 'g:')ax.plot(df2['matched'], 'k:')ax.set_xlabel('Ordered index')ax.set_ylabel('Matching observations')ax.set_title('Verification on a 6-observation chain.')ax2 = ax.twinx()ax2.plot(df['counts'], 'r', lw=3)ax2.plot(df2['counts'], 'k', lw=3)ax2.set_ylabel('Frequency of occurrence [%]')ax.legend(['trained', 'initialized'])ax2.legend(['trained', 'initialized'])plt.grid()plt.show()",
  "translation": "hml_rand = HiddenMarkovLayer.initialize（状态，可观察值）hmm_rand = HiddenMarkovModel（hml_rand）RUNS = 100000T = 5chains_rand = RUNS * [0] for i in range（len（chains_rand））：chain_rand = hmm_rand.layer.run（T）[0 ] chains_rand [i] ='-'。join（chain_rand）df2 = pd.DataFrame（pd.Series（chains_rand）.value_counts（），columns = ['counts']）。reset_index（）。rename（columns = {' index'：'chain'}）df2 = pd.merge（df2，df2 ['chain']。str.split（'-'，expand = True），left_index = True，right_index = True）s = []对于我在范围（T + 1）中：s.append（df2.apply（lambda x：x [i] ==观察值[i]，轴= 1））df2 ['matched'] = pd.concat（s，axis = 1）.sum（axis = 1）df2 ['counts'] = df2 ['counts'] / RUNS * 100df2 = df2.drop（columns = ['chain']）fig，ax = plt.subplots（1，1 ，figsize =（14，6））ax.plot（df ['matched']，'g：'）ax.plot（df2 ['matched']，'k：'）ax.set_xlabel（'Ordered index'） ax.set_ylabel（'匹配观察'）ax.set_title（'对6个观察链的验证。'）ax2 = ax.twinx（）ax2.plot（df ['counts']，'r'，lw = 3） ax2.plot（df2 ['counts']，'k'，lw = 3）ax2.set_ylabel（发生频率[％]'）ax.legend（['trained'，'initialized']）ax2.legend（['trained'，'initialized']）plt.grid（）plt。显示（）"
}, {
  "tag": "FIGURE",
  "image": "https://miro.medium.com/max/60/0*xAdlWm6TemJ-jrcl.png?q=20",
  "caption": "Figure 4. Result after training of the model. The dotted lines represent the matched sequences. The lines represent the frequency of occurrence for a particular sequence: trained model (red) and freshly initialized (black). The initialized results in almost perfect uniform distribution of sequences, while the trained model gives a strong preference towards the observable sequence.",
  "type": "image",
  "file": "0!xAdlWm6TemJ-jrcl.png"
}, {
  "tag": "P",
  "text": "It seems we have successfully implemented the training procedure. If we look at the curves, the initialized-only model generates observation sequences with almost equal probability. It’s completely random. However, the trained model gives sequences that are highly similar to the one we desire with much higher frequency. Despite the genuine sequence gets created in only 2% of total runs, the other similar sequences get generated approximately as often.",
  "translation": "看来我们已经成功实施了培训程序。 如果我们看这些曲线，则仅初始化模型将以几乎相等的概率生成观察序列。 这是完全随机的。 但是，经过训练的模型给出的序列与我们期望的序列高度相似，并且频率更高。 尽管真正的序列仅在总运行中的2％中创建，但其他相似序列的生成频率却差不多。"
}, {
  "tag": "H1",
  "text": "Conclusion",
  "translation": "结论"
}, {
  "tag": "P",
  "text": "In this article, we have presented a step-by-step implementation of the Hidden Markov Model. We have created the code by adapting the first principles approach. More specifically, we have shown how the probabilistic concepts that are expressed through equations can be implemented as objects and methods. Finally, we demonstrated the usage of the model with finding the score, uncovering of the latent variable chain and applied the training procedure."
}, {
  "tag": "P",
  "text": "PS. I apologise for the poor rendering of the equations here. Basically, I needed to do it all manually. However, please feel free to read this article on my home blog. There, I took care of it ;)",
  "translation": "PS。 对于此处的等式表示不佳，我深表歉意。 基本上，我需要手动完成所有操作。 但是，请随时在我的家庭博客上阅读此文章。 在那里，我照顾好了;）"
}, {
  "tag": "PRE",
  "text": "(本文翻译自Oleg Żero的文章《Hidden Markov Model — Implemented from scratch》，参考：https://towardsdatascience.com/hidden-markov-model-implemented-from-scratch-72865bda430e)",
  "translation": "（本文翻译自OlegŻero的文章《隐马尔可夫模型-从头开始实施》，参考：https：//towardsdatascience.com/hidden-markov-model-implemented-from-scratch-72865bda430e）"
}]